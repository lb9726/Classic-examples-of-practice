《大数据时代（精华版）》
作者：[英]维克托·迈尔·舍恩伯格著，周涛译

内容简介：    《大数据时代》是国外大数据研究的先河之作，本书作者维克托·迈尔·舍恩伯格被誉为“大数据商业应用第一人”。本书前瞻性地指出，大数据带来的信息风暴正在变革我们的生活、工作和思维，大数据开启了一次重大的时代转型，并用三个部分讲述了大数据时代的思维变革、商业变革和管理变革。"


引言 一场生活、工作与思维的大变革 大数据，变革公共卫生

    2009年出现了一种新的流感病毒。这种甲型h1n1流感结合了导致禽流感和猪流感的病毒的特点，在短短几周之内迅速传播开来。全球的公共卫生机构都担心一场致命的流行病即将来袭。有的评论家甚至警告说，可能会爆发大规模流感，类似于1918年在西班牙爆发的、影响了5亿人口并夺走了数千万人『性』命的大规模流感。更糟糕的是，我们还没有研发出对抗这种新型流感病毒的疫苗。公共卫生专家能做的只是减慢它传播的速度。但要做到这一点，他们必须先知道这种流感出现在哪里。

    美国，和所有其他国家一样，都要求医生在发现新型流感病例时告知疾病控制与预防中心（cdc）。但由于人们可能患病多日实在受不了了才会去医院，同时这个信息传达回疾控中心也需要时间，因此，通告新流感病例时往往会有一两周的延迟。而且，疾控中心每周只进行一次数据汇总。然而，对于一种飞速传播的疾病，信息滞后两周的后果将是致命的。这种滞后导致公共卫生机构在疫情爆发的关键时期反而无所适从。

    在甲型h1n1流感爆发的几周前，互联网巨头谷歌公司的工程师们在《自然》杂志上发表了一篇引人注目的论文。它令公共卫生官员们和计算机科学家们感到震惊。文中解释了谷歌为什么能够预测冬季流感的传播：不仅是全美范围的传播，而且可以具体到特定的地区和州。谷歌通过观察人们在网上的搜索记录来完成这个预测，而这种方法以前一直是被忽略的。谷歌保存了多年来所有的搜索记录，而且每天都会收到来自全球超过30亿条的搜索指令，如此庞大的数据资源足以支撑和帮助它完成这项工作。

    发现能够通过人们在网上检索的词条辨别出其是否感染了流感后，谷歌公司把5000万条美国人最频繁检索的词条和美国疾控中心在2003年至2008年间季节『性』流感传播时期的数据进行了比较。其他公司也曾试图确定这些相关的词条，但是他们缺乏像谷歌公司一样庞大的数据资源、处理能力和统计技术。

    虽然谷歌公司的员工猜测，特定的检索词条是为了在网络上得到关于流感的信息，如“哪些是治疗咳嗽和发热的『药』物”，但是找出这些词条并不是重点，他们也不知道哪些词条更重要，更关键的是，他们建立的系统并不依赖于这样的语义理解。他们设立的这个系统唯一关注的就是特定检索词条的频繁使用与流感在时间和空间上的传播之间的联系。谷歌公司为了测试这些检索词条，总共处理了4.5亿个不同的数字模型。在将得出的预测与2007年、2008年美国疾控中心记录的实际流感病例进行对比后，谷歌公司发现，他们的软件发现了45条检索词条的组合，一旦将它们用于一个数学模型，他们的预测与官方数据的相关『性』高达97%。和疾控中心一样，他们也能判断出流感是从哪里传播出来的，而且他们的判断非常及时，不会像疾控中心一样要在流感爆发一两周之后才可以做到。

    所以，2009年甲型h1n1流感爆发的时候，与习惯『性』滞后的官方数据相比，谷歌成为了一个更有效、更及时的指示标。公共卫生机构的官员获得了非常有价值的数据信息。惊人的是，谷歌公司的方法甚至不需要分发口腔试纸和联系医生——它是建立在大数据的基础之上的。这是当今社会所独有的一种新型能力：以一种前所未有的方式，通过对海量数据进行分析，获得有巨大价值的产品和服务，或深刻的洞见。基于这样的技术理念和数据储备，下一次流感来袭的时候，世界将会拥有一种更好的预测工具，以预防流感的传播。

     

引言 一场生活、工作与思维的大变革 大数据，变革商业

    大数据不仅改变了公共卫生领域，整个商业领域都因为大数据而重新洗牌。购买飞机票就是一个很好的例子。

    2003年，奥伦·埃齐奥尼（oren etzioni）准备乘坐从西雅图到洛杉矶的飞机去参加弟弟的婚礼。他知道飞机票越早预订越便宜，于是他在这个大喜日子来临之前的几个月，就在网上预订了一张去洛杉矶的机票。在飞机上，埃齐奥尼好奇地问邻座的乘客花了多少钱购买机票。当得知虽然那个人的机票比他买得更晚，但是票价却比他便宜得多时，他感到非常气愤。于是，他又询问了另外几个乘客，结果发现大家买的票居然都比他的便宜。

    对大多数人来说，这种被敲竹杠的感觉也许会随着他们走下飞机而消失。然而，埃齐奥尼是美国最有名的计算机专家之一，从他担任华盛顿大学人工智能项目的负责人开始，他创立了许多在今天看来非常典型的大数据公司，而那时候还没有人提出“大数据”这个概念。

    1994年，埃齐奥尼帮助创建了最早的互联网搜索引擎metacrawler，该引擎后来被infospace公司收购。他联合创立了第一个大型比价网站netbot，后来把它卖给了excite公司。他创立的从文本中挖掘信息的公司clearforest则被路透社收购了。在他眼中，世界就是一系列的大数据问题，而且他认为他有能力解决这些问题。作为哈佛大学首届计算机科学专业的本科毕业生，自1986年毕业以来，他也一直致力于解决这些问题。

    飞机着陆之后，埃齐奥尼下定决心要帮助人们开发一个系统，用来推测当前网页上的机票价格是否合理。作为一种商品，同一架飞机上每个座位的价格本来不应该有差别。但实际上，价格却千差万别，其中缘由只有航空公司自己清楚。

    埃齐奥尼表示，他不需要去解开机票价格差异的奥秘。他要做的仅仅是预测当前的机票价格在未来一段时间内会上涨还是下降。这个想法是可行的，但『操』作起来并不是那么简单。这个系统需要分析所有特定航线机票的销售价格并确定票价与提前购买天数的关系。

    如果一张机票的平均价格呈下降趋势，系统就会帮助用户做出稍后再购票的明智选择。反过来，如果一张机票的平均价格呈上涨趋势，系统就会提醒用户立刻购买该机票。换言之，这是埃齐奥尼针对9000米高空开发的一个加强版的信息预测系统。这确实是一个浩大的计算机科学项目。不过，这个项目是可行的。于是，埃齐奥尼开始着手启动这个项目。

    埃齐奥尼创立了一个预测系统，它帮助虚拟的乘客节省了很多钱。这个预测系统建立在41天内价格波动产生的12000个价格样本基础之上，而这些信息都是从一个旅游网站上搜集来的。这个预测系统并不能说明原因，只能推测会发生什么。也就是说，它不知道是哪些因素导致了机票价格的波动。机票降价是因为很多没卖掉的座位、季节『性』原因，还是所谓的周六晚上不出门，它都不知道。这个系统只知道利用其他航班的数据来预测未来机票价格的走势。“买还是不买，这是一个问题。”埃齐奥尼沉思着。他给这个研究项目取了一个非常贴切的名字，叫“哈姆雷特”。

    这个小项目逐渐发展成为一家得到了风险投资基金支持的科技创业公司，名为farecast。通过预测机票价格的走势以及增降幅度，farecast票价预测工具能帮助消费者抓住最佳购买时机，而在此之前还没有其他网站能让消费者获得这些信息。

    这个系统为了保障自身的透明度，会把对机票价格走势预测的可信度标示出来，供消费者参考。系统的运转需要海量数据的支持。为了提高预测的准确『性』，埃齐奥尼找到了一个行业机票预订数据库。有了这个数据库，系统进行预测时，预测的结果就可以基于美国商业航空产业中，每一条航线上每一架飞机内的每一个座位一年内的综合票价记录而得出。如今，farecast已经拥有惊人的约2000亿条飞行数据记录。利用这种方法，farecast为消费者节省了一大笔钱。

    棕『色』的头发，『露』齿的笑容，无邪的面孔，这就是奥伦·埃齐奥尼。他看上去完全不像是一个会让航空业损失数百万潜在收入的人。但事实上，他的目光放得更长远。2008年，埃齐奥尼计划将这项技术应用到其他领域，比如宾馆预订、二手车购买等。只要这些领域内的产品差异不大，同时存在大幅度的价格差和大量可运用的数据，就都可以应用这项技术。但是在他实现计划之前，微软公司找上了他并以1.1亿美元的价格收购了farecast公司。而后，这个系统被并入必应搜索引擎。

    farecast是大数据公司的一个缩影，也代表了当今世界发展的趋势。五年或者十年之前，奥伦·埃齐奥尼是无法成立这样的公司的。他说：“这是不可能的。”那时候他所需要的计算机处理能力和存储能力太昂贵了！虽说技术上的突破是这一切得以发生的主要原因，但也有一些细微而重要的改变正在发生，特别是人们关于如何使用数据的理念。

     

引言 一场生活、工作与思维的大变革 大数据，变革思维

    人们不再认为数据是静止和陈旧的。但在以前，一旦完成了收集数据的目的之后，数据就会被认为已经没有用处了。比方说，在飞机降落之后，票价数据就没有用了（对谷歌而言，则是一个检索命令完成之后）。

    信息社会所带来的好处是显而易见的：每个人口袋里都揣有一部手机，每台办公桌上都放有一台电脑，每间办公室内都拥有一个大型局域网。但是，信息本身的用处却并没有如此引人注目。半个世纪以来，随着计算机技术全面融入社会生活，信息爆炸已经积累到了一个开始引发变革的程度。它不仅使世界充斥着比以往更多的信息，而且其增长速度也在加快。信息总量的变化还导致了信息形态的变化——量变引发了质变。最先经历信息爆炸的学科，如天文学和基因学，创造出了“大数据”这个概念。如今，这个概念几乎应用到了所有人类致力于发展的领域中。

    大数据并非一个确切的概念。最初，这个概念是指需要处理的信息量过大，已经超出了一般电脑在处理数据时所能使用的内存量，因此工程师们必须改进处理数据的工具。这导致了新的处理技术的诞生，例如谷歌的mapreduce和开源hadoop平台（最初源于雅虎）。这些技术使得人们可以处理的数据量大大增加。更重要的是，这些数据不再需要用传统的数据库表格来整齐地排列——一些可以消除僵化的层次结构和一致『性』的技术也出现了。同时，因为互联网公司可以收集大量有价值的数据，而且有利用这些数据的强烈的利益驱动力，所以互联网公司就顺理成章地成为最新处理技术的领头实践者。它们甚至超过了很多有几十年经验的线下公司，成为新技术的领衔使用者。

    今天，一种可能的方式是，亦是本书采取的方式，认为大数据是人们在大规模数据的基础上可以做到的事情，而这些事情在小规模数据的基础上是无法完成的。大数据是人们获得新的认知，创造新的价值的源泉；大数据还是改变市场、组织机构，以及『政府』与公民关系的方法。

     

引言 一场生活、工作与思维的大变革 大数据，开启重大的时代转型

    大数据开启了一次重大的时代转型。与其他新技术一样，大数据也必然要经历硅谷臭名昭著的技术成熟度曲线：经过新闻媒体和学术会议的大肆宣传之后，新技术趋势一下子跌到谷底，许多数据创业公司变得岌岌可危。当然，不管是过热期还是幻想破灭期，都非常不利于我们正确理解正在发生的变革的重要『性』。

    就像望远镜能够让我们感受宇宙，显微镜能够让我们观测微生物，这种能够收集和分析海量数据的新技术将帮助我们更好地理解世界——这种理解世界的新方法我们现在才意识到。本书旨在如实表达出大数据的内涵，而不会过分热捧它。当然，真正的革命并不在于分析数据的机器，而在于数据本身和我们如何运用数据。

    2003年，人类第一次破译人体基因密码的时候，辛苦工作了十年才完成了三十亿对碱基对的排序。大约十年之后，世界范围内的基因仪每15分钟就可以完成同样的工作。在金融领域，美国股市每天的成交量高达70亿股。而其中三分之二的交易都是由建立在算法公式上的计算机程序完成的。这些程序运用海量数据来预测利益和降低风险。

    互联网公司更是要被数据淹没了。谷歌公司每天要处理超过24拍（等于2的50次方）字节的数据，这意味着其每天的数据处理量是美国国家图书馆所有纸质出版物所含数据量的上千倍。facebook这个创立时间不足十年的公司，每天更新的照片量超过1000万张，每天人们在网站上点击“喜欢”（like）按钮或者写评论次数大约有三十亿次，这就为facebook公司挖掘用户喜好提供了大量的数据线索。与此同时，谷歌子公司youtube每月接待多达8亿的访客，平均每一秒钟就会有一段长度在一小时以上的视频上传。twitter上的信息量几乎每年翻一倍，截至2012年，每天都会发布超过4亿条微博。

    从科学研究到医疗保险，从银行业到互联网，各个不同的领域都在讲述着一个类似的故事，那就是爆发式增长的数据量。这种增长超过了我们创造机器的速度，甚至超过了我们的想象。

    我们周围到底有多少数据？增长的速度有多快？许多人试图测量出一个确切的数字。尽管测量的对象和方法有所不同，但他们都获得了不同程度的成功。南加利福尼亚大学安嫩伯格通信学院的马丁·希尔伯特（martin hilbert）进行了一个比较全面的研究，他试图得出人类所创造、存储和传播的一切信息的确切数目。他的研究范围不仅包括书籍、图画、电子邮件、照片、音乐、视频（模拟和数字），还包括电子游戏、电话、汽车导航和信件。马丁·希尔伯特还以收视率和收听率为基础，对电视、电台这些广播媒体进行了研究。

    有趣的是，在2007年，只有7%是存储在报纸、书籍、图片等媒介上的模拟数据，其余全部是数字数据。但在不久之前，情况却完全不是这样的。虽然1960年就有了“信息时代”和“数字村镇”的概念，但实际上，这些概念仍然是相当新颖的。甚至在2000年的时候，数字存储信息仍只占全球数据量的四分之一；当时，另外四分之三的信息都存储在报纸、胶片、黑胶唱片和盒式磁带这类媒介上。

    早期数字信息的数量是不多的。对于长期在网上冲浪和购书的人来说，那只是一个微小的部分。事实上，在1986年的时候，世界上约40%的计算机技术都被运用在便携计算机上，那时候，所有个人电脑的处理能力之和都没有便携计算机高。但是因为数字数据的快速增长，整个局势很快就颠倒过来了。按照希尔伯特的说法，数字数据的数量每三年多就会翻一倍。相反，模拟数据的数量则基本上没有增加。

    事情真的在快速发展。人类存储信息量的增长速度比世界经济的增长速度快4倍，而计算机数据处理能力的增长速度则比世界经济的增长速度快9倍。难怪人们会抱怨信息过量，因为每个人都受到了这种极速发展的冲击。

    把眼光放远一点，我们可以把时下的信息洪流与1439年前后古登堡发明印刷机时造成的信息爆炸相对比。历史学家伊丽莎白·爱森斯坦（elizabeth eisenstein）发现，1453—1503年，这50年之间大约有800万本书籍被印刷，比1200年之前君士坦丁堡建立以来整个欧洲所有的手抄书还要多。换言之，欧洲的信息存储量花了50年才增长了一倍（当时的欧洲还占据了世界上大部分的信息存储份额），而如今大约每三年就能增长一倍。

    这种增长意味着什么呢？彼特·诺维格（peter norvig）是谷歌的人工智能专家，也曾任职于美国宇航局喷气推进实验室，他喜欢把这种增长与图画进行类比。首先，他要我们想想来自法国拉斯科洞『穴』壁画上的标志『性』的马。这些画可以追溯到一万七千年之前的旧石器时代。然后，再想想一张马的照片，想想毕加索的画也可以，看起来和那些洞『穴』壁画没有多大的差别。事实上，毕加索看到那些洞『穴』壁画的时候就曾开玩笑说：“自那以后，我们就再也没有创造出什么东西了。”

    他的话既正确又不完全正确。你回想一下壁画上的那匹马。当时要画一幅马的画需要花费很久的时间，而现在不需要那么久了。这就是一种改变，虽然改变的可能不是最核心的部分——毕竟这仍然是一幅马的图像。但是诺维格说，想象一下，现在我们能每秒钟播放24幅不同形态的马的图片，这就是一种由量变导致的质变：一部电影与一幅静态的画有本质上的区别！大数据也一样，量变导致质变。物理学和生物学都告诉我们，当我们改变规模时，事物的状态有时也会发生改变。

    我们就以纳米技术来为例。纳米技术就是让一切变小而不是变大。其原理就是当事物到达分子的级别时，它的物理『性』质就会发生改变。一旦你知道这些新的『性』质，你就可以用同样的原料来做以前无法做的事情。铜本来是用来导电的物质，但它一旦到达纳米级别就不能在磁场中导电了。银离子具有抗菌『性』，但当它以分子形式存在的时候，这种『性』质会消失。一旦到达纳米级别，金属可以变得柔软，陶土可以具有弹『性』。同样，当我们增加所利用的数据量时，我们就可以做很多在小数据量的基础上无法完成的事情。

    有时候，我们认为约束我们生活的那些限制，对于世间万物都有着同样的约束力。事实上，尽管规律相同，但是我们能够感受到的约束，很可能只对我们这样尺度的事物起作用。对于人类来说，唯一一个最重要的物理定律便是万有引力定律。这个定律无时无刻不在控制着我们。但对于细小的昆虫来说，重力是无关紧要的。对它们而言，物理宇宙中有效的约束是地表张力，这个张力可以让它们在水上自由行走而不会掉下去。但人类对于地表张力毫不在意。

    对于万有引力产生的约束效果而言，生物体的大小是非常重要的。类似地，对于信息而言，规模也是非常重要的。谷歌能够几近完美地给出和基于大量真实病例信息所得到的流感情况一致的结果，而且几乎是实时的，比疾控中心快多了。同样，farecast可以预测机票价格的波动，从而让消费者真正在经济上获利。它们之所以如此给力，都因为存在供其分析的数千亿计的数据项。

    尽管我们仍处于大数据时代来临的前夕，但我们的日常生活已经离不开它了。垃圾邮件过滤器可以自动过滤垃圾邮件，尽管它并不知道“发#票#销#售”是“发票销售”的一种变体。交友网站根据个人的『性』格与之前成功配对的情侣之间的关联来进行新的配对。具有“自动改正”功能的智能手机通过分析我们以前的输入，将个『性』化的新单词添加到手机词典里。然而，对于这些数据的利用还仅仅只是一个开始。从可以自动转弯和刹车的汽车，到ibm沃特森超级电脑在游戏节目《危险边缘》（jeopardy）中打败人类来看，这项技术终将会改变我们所居住的星球的许多东西。

     

引言 一场生活、工作与思维的大变革 预测，大数据的核心

    大数据的核心就是预测。它通常被视为人工智能的一部分，或者更确切地说，被视为一种机器学习。但是这种定义是有误导『性』的。大数据不是要教机器像人一样思考。相反，它是把数学算法运用到海量的数据上来预测事情发生的可能『性』。一封邮件被作为垃圾邮件过滤掉的可能『性』，输入的“teh”应该是“the”的可能『性』，从一个人『乱』穿马路时行进的轨迹和速度来看他能及时穿过马路的可能『性』，都是大数据可以预测的范围。当然，如果一个人能及时穿过马路，那么他『乱』穿马路时，车子就只需要稍稍减速就好。但是这些预测系统之所以能够成功，关键在于它们是建立在海量数据的基础之上的。此外，随着系统接收到的数据越来越多，通过记录找到的最好的预测与模式，可以对系统进行改进。

    在不久的将来，世界许多现在单纯依靠人类判断力的领域都会被计算机系统所改变甚至取代。计算机系统可以发挥作用的领域远远不止驾驶和交友，还有更多更复杂的任务。别忘了，亚马逊可以帮我们推荐想要的书，谷歌可以为关联网站排序，facebook知道我们的喜好，而linkedin可以猜出我们认识谁。当然，同样的技术也可以运用到疾病诊断、推荐治疗措施，甚至是识别潜在犯罪分子上。

    就像互联网通过给计算机添加通信功能而改变了世界，大数据也将改变我们生活中最重要的方面，因为它为我们的生活创造了前所未有的可量化的维度。大数据已经成为了新发明和新服务的源泉，而更多的改变正蓄势待发。

     

引言 一场生活、工作与思维的大变革 大数据，大挑战

    大数据的核心代表着我们分析信息时的三个转变。这些转变将改变我们理解和组建社会的方法。

    第一个转变就是，在大数据时代，我们可以分析更多的数据，有时候甚至可以处理和某个特别现象相关的所有数据，而不再依赖于随机采样。这部分内容将在第1章阐述。19世纪以来，当面临大量数据时，社会都依赖于采样分析。但是采样分析是信息缺乏时代和信息流通受限制的模拟数据时代的产物。以前我们通常把这看成了理所当然的限制，但高『性』能数字技术的流行让我们意识到，这其实是一种人为的限制。与局限在小数据范围相比，使用一切数据为我们带来了更高的精确『性』，也让我们看到了一些以前无法发现的细节——大数据让我们更清楚地看到了样本无法揭示的细节信息。

    第二个改变就是，研究数据如此之多，以至于我们不再热衷于追求精确度。这部分内容将在第2章阐述。当我们测量事物的能力受限时，关注最重要的事情和获取最精确的结果是可取的。如果购买者不知道牛群里有80头牛还是100头牛，那么交易就无法进行。直到今天，我们的数字技术依然建立在精准的基础上。我们假设只要电子数据表格把数据排序，数据库引擎就可以找出和我们检索的内容完全一致的检索记录。

    这种思维方式适用于掌握“小数据量”的情况，因为需要分析的数据很少，所以我们必须尽可能精准地量化我们的记录。在某些方面，我们已经意识到了差别。例如，一个小商店在晚上打烊的时候要把收银台里的每分钱都数清楚，但是我们不会、也不可能用“分”这个单位去精确计算国民生产总值。随着规模的扩大，对精确度的痴『迷』将减弱。

    达到精确需要有专业的数据库。针对小数据量和特定事情，追求精确『性』依然是可行的，比如一个人的银行账户上是否有足够的钱开具支票。但是，在这个大数据时代，在很多时候，追求精确度已经变得不可行，甚至不受欢迎了。当我们拥有海量即时数据时，绝对的精准不再是我们追求的主要目标。

    大数据纷繁多样，优劣掺杂，分布在全球多个服务器上。拥有了大数据，我们不再需要对一个现象刨根究底，只要掌握大体的发展方向即可。当然，我们也不是完全放弃了精确度，只是不再沉『迷』于此。适当忽略微观层面上的精确度会让我们在宏观层面拥有更好的洞察力。

    第三个转变因前两个转变而促成，即我们不再热衷于寻找因果关系。这部分内容将在第3章阐述。寻找因果关系是人类长久以来的习惯。即使确定因果关系很困难而且用途不大，人类还是习惯『性』地寻找缘由。相反，在大数据时代，我们无须再紧盯事物之间的因果关系，而应该寻找事物之间的相关关系，这会给我们提供非常新颖且有价值的观点。相关关系也许不能准确地告知我们某件事情为何会发生，但是它会提醒我们这件事情正在发生。在许多情况下，这种提醒的帮助已经足够大了。

    如果电子医疗记录显示橙汁和阿司匹林的特定组合可以治疗癌症，那么找出具体的致病原因就没有这种治疗方法本身来得重要。同样，只要我们知道什么时候是买机票的最佳时机，就算不知道机票价格疯狂变动的原因也无所谓了。大数据告诉我们“是什么”而不是“为什么”。在大数据时代，我们不必知道现象背后的原因，我们只要让数据自己发声。

    我们不再需要在还没有收集数据之前，就把我们的分析建立在早已设立的少量假设的基础之上。让数据发声，我们会注意到很多以前从来没有意识到的联系的存在。

    例如，对冲基金通过剖析社交网络twitter上的数据信息来预测股市的表现；亚马逊和奈飞（netflix）根据用户在其网站上的类似查询来进行产品推荐；twitter，facebookllinkedin通过用户的社交网络图来得知用户的喜好。

    当然，人类从数千年前就开始分析数据。古代美索不达米亚平原的记账人员为了有效地跟踪记录信息发明了书写。自从圣经时代开始，『政府』就通过进行人口普查来建立大型的国民数据库。两百多年来，精算师们也一直通过搜集大量的数据来进行风险规避。

    模拟时代的数据收集和分析极其耗时耗力，新问题的出现通常要求我们重新收集和分析数据。数字化的到来使得数据管理效率又向前迈出了重要的一步。数字化将模拟数据转换成计算机可以读取的数字数据，使得存储和处理这些数据变得既便宜又容易，从而大大提高了数据管理效率。过去需要几年时间才能完成的数据搜集，现在只要几天就能完成。但是，光有改变还远远不够。数据分析者太沉浸于模拟数据时代的设想，即数据库只有单一的用途和价值，而正是我们使用的技术和方法加深了这种偏见。虽然数字化是促成向大数据转变的重要原因，但仅有计算机的存在却不足以实现大数据。

    我们没有办法准确描述现在正在发生的一切，但是在第4章即将提到的“数据化”概念可以帮助我们大致了解这次变革。数据化意味着我们把一切都透明化，甚至包括很多我们以前认为和“信息”根本搭不上边的事情。比方说，一个人所在的位置、引擎的振动、桥梁的承重等。我们要通过量化的方法把这些内容转化为数据。这就使得我们可以尝试许多以前无法做到的事情，如根据引擎的散热和振动来预测引擎是否会出现故障。这样，我们就激发出了这些数据此前未被挖掘的潜在价值。

    大数据时代开启了一场寻宝游戏，而人们对于数据的看法以及对于由因果关系向相关关系转化时释放出的潜在价值的态度，正是主宰这场游戏的关键。新兴技术工具的使用使这一切成为可能。宝贝不止一件，每个数据集内部都隐藏着某些未被发掘的价值。这场发掘和利用数据价值的竞赛正开始在全球上演。

    第5章和第6章将讲述大数据如何改变了商业、市场和社会的本质。20世纪，价值已经从实体基建转变为无形财产，从土地和工厂转变为品牌和产权。如今，一个新的转变正在进行，那就是电脑存储和分析数据的方法取代电脑硬件成为了价值的源泉。数据成为了有价值的公司资产、重要的经济投入和新型商业模式的基石。虽然数据还没有被列入企业的资产负债表，但这只是一个时间问题。

    虽然有些数据处理技术已经出现了一段时间，但是它们只为调查局、研究所和世界上的一些巨头公司所掌握。沃尔玛和美国第一资本银行（capitalone）率先将大数据运用在了零售业和银行业，因此改变了整个行业。如今这种技术大多都实现了大众化。

    大数据对个人的影响是最惊人的。在一个可能『性』和相关『性』占主导地位的世界里，专业『性』变得不那么重要了。行业专家不会消失，但是他们必须与数据表达的信息进行博弈。如同在电影《点球成金》（moneyball）里，棒球星探们在统计学家面前相形见绌——直觉的判断被迫让位于精准的数据分析。这将迫使人们调整在管理、决策、人力资源和教育方面的传统理念。

    我们大部分的习俗和惯例都建立在一个预设好的立场上，那就是我们用来进行决策的信息必须是少量、精确并且至关重要的。但是，当数据量变大、数据处理速度加快，而且数据变得不那么精确时，之前的那些预设立场就不复存在了。此外，因为数据量极为庞大，最后做出决策的将是机器而不是人类自己。第7章将会讨论大数据的负面影响。

    在了解和监视人类的行为方面，社会已经有了数千年的经验。但是，如何来监管一个算法系统呢？在信息化时代的早期，有一些政策专家就看到了信息化给人们的隐私权带来的威胁，社会也已经建立起了庞大的规则体系来保障个人的信息安全。但是在大数据时代，这些规则都成了无用的马其诺防线。人们自愿在网络上分享信息，而这种分享的能力成为了网络服务的一个中心特征，而不再是一个需要规避的薄弱点了。

    对我们而言，危险不再是隐私的泄『露』，而是被预知的可能『性』——这些能预测我们可能生病、拖欠还款和犯罪的算法会让我们无法购买保险、无法贷款、甚至在实施犯罪前就被预先逮捕。显然，统计把大数据放在了首位，但即便如此，个人意志是否应该凌驾于大数据之上呢？就像出版印刷行业的发展推动国家立法保护言论自由（在此之前没有出台类似法律的必要，因为没有太多的言论需要保护），大数据时代也需要新的规章制度来保卫权势面前的个人权利。

    『政府』机构和社会在控制和处理数据的方法上必须有多方位的改变。不可否认，我们进入了一个用数据进行预测的时代，虽然我们可能无法解释其背后的原因。如果一个医生只要求病人遵从医嘱，却没法说明医学干预的合理『性』的话，情况会怎么样呢？实际上，这是依靠大数据取得病理分析的医生们一定会做的事情。还有司法系统的“合理证据”是不是应该改为“可能证据”呢？如果真是这样，会对人类自由和尊严产生什么影响呢？

    我们在大数据时代倡导的一系列规范将在第8章进行介绍。这些规范建立在我们很熟悉的“小数据”时代发展并保留下来的规范的基础之上。新环境要求旧规范与时俱进。

    大数据标志着人类在寻求量化和认识世界的道路上前进了一大步。过去不可计量、存储、分析和共享的很多东西都被数据化了。拥有大量的数据和更多不那么精确的数据为我们理解世界打开了一扇新的大门。社会因此放弃了寻找因果关系的传统偏好，开始挖掘相关关系的好处。

    寻找原因是一种现代社会的一神论，大数据推翻了这个论断。但我们又陷入了一个历史的困境，那就是我们活在一个“上帝已死”的时代。也就是说，我们曾经坚守的信念动摇了。讽刺的是，这些信念正在被“更好”的证据所取代。那么，从经验中得来的与证据相矛盾的直觉、信念和『迷』惘应该充当什么角『色』呢？当世界由探求因果关系变成挖掘相关关系，我们怎样才能既不损坏建立在因果推理基础之上的社会繁荣和人类进步的基石，又取得实际的进步呢？本书意在解释我们身在何处，我们从何而来，并且提供当下亟需的指导，以应对眼前的利益和危险。

     

第一部分 大数据时代的思维变革 01 更多：不是随机样本，而是全体数据

    让数据“发声”

    “大数据”全在于发现和理解信息内容及信息与信息之间的关系，然而直到最近，我们对此似乎还是难以把握。ibm的资深“大数据”专家杰夫·乔纳斯（jeff jonas）提出要让数据“说话”。从某种层面上来说，这听起来很平常。人们使用数据已经有相当长一段时间了，无论是日常进行的大量非正式观察，还是过去几个世纪里在专业层面上用高级算法进行的量化研究，都与数据有关。

    在数字化时代，数据处理变得更加容易、更加快速，人们能够在瞬间处理成千上万的数据。但当我们谈论能“说话”的数据时，我们指的远远不止这些。

    实际上，大数据与三个重大的思维转变有关，这三个转变是相互联系和相互作用的。

    首先，要分析与某事物相关的所有数据，而不是依靠分析少量的数据样本。

    其次，我们乐于接受数据的纷繁复杂，而不再追求精确『性』。

    最后，我们的思想发生了转变，不再探求难以捉『摸』的因果关系，转而关注事物的相关关系。

    本章就将介绍第一个转变：利用所有的数据，而不再仅仅依靠一小部分数据。

    很长一段时间以来，准确分析大量数据对我们而言都是一种挑战。过去，因为记录、储存和分析数据的工具不够好，我们只能收集少量数据进行分析，这让我们一度很苦恼。为了让分析变得简单，我们会把数据量缩减到最少。这是一种无意识的自省：我们把与数据交流的困难看成是自然的，而没有意识到这只是当时技术条件下的一种人为的限制。如今，技术条件已经有了非常大的提高，虽然人类可以处理的数据依然是有限的，也永远是有限的，但是我们可以处理的数据量已经大大地增加，而且未来会越来越多。

    在某些方面，我们依然没有完全意识到自己拥有了能够收集和处理更大规模数据的能力。我们还是在信息匮乏的假设下做很多事情，建立很多机构组织。我们假定自己只能收集到少量信息，结果就真的如此了。这是一个自我实现的过程。我们甚至发展了一些使用尽可能少的信息的技术。别忘了，统计学的一个目的就是用尽可能少的数据来证实尽可能重大的发现。事实上，我们形成了一种习惯，那就是在我们的制度、处理过程和激励机制中尽可能地减少数据的使用。为了理解大数据时代的转变意味着什么，我们需要首先回顾一下过去。

    小数据时代的随机采样，最少的数据获得最多的信息

    直到最近，私人企业和个人才拥有了大规模收集和分类数据的能力。在过去，这是只有教会或者『政府』才能做到的。当然，在很多国家，教会和『政府』是等同的。有记载的、最早的计数发生在公元前8000年，当时苏美尔的商人用黏土珠来记录出售的商品。大规模的计数则是『政府』的事情。数千年来，『政府』都试图通过收集信息来管理国民。

    以人口普查为例。据说古代埃及曾进行过人口普查，《旧约》和《新约》中对此都有所提及。那次由奥古斯都凯撒主导实施的人口普查，提出了“每个人都必须纳税”，这使得约瑟夫和玛丽搬到了耶稣的出生地伯利恒。1086年的《末日审判书》（the doomsday book）对当时英国的人口、土地和财产做了一个前所未有的全面记载。皇家委员穿越整个国家对每个人、每件事都做了记载，后来这本书用《圣经》中的《末日审判书》命名，因为每个人的生活都被赤『裸』『裸』地记载下来的过程就像接受“最后的审判”一样。

    然而，人口普查是一项耗资且费时的事情。国王威廉一世（king william i）在他发起的《末日审判书》完成之前就去世了。但是，除非放弃收集信息，否则在当时没有其他办法。尽管如此，当时收集的信息也只是一个大概情况，实施人口普查的人也知道他们不可能准确记录下每个人的信息。实际上，“人口普查”这个词来源于拉丁语的“censere”，意思就是推测、估算。

    三百多年前，一个名叫约翰·格朗特（john graunt）的英国缝纫用品商提出了一个很有新意的方法。他采用了一个新方法推算出鼠疫时期伦敦的人口数，这种方法就是后来的统计学。这个方法不需要一个人一个人地计算。虽然这个方法比较粗糙，但采用这个方法，人们可以利用少量有用的样本信息来获取人口的整体情况。

    虽然后来证实他能够得出正确的数据仅仅是因为运气好，但在当时他的方法大受欢迎。样本分析法一直都有较大的漏洞，因此无论是进行人口普查还是其他大数据类的任务，人们还是一直使用具体计数这种“野蛮”的方法。

    考虑到人口普查的复杂『性』以及耗时耗费的特点，『政府』极少进行普查。古罗马人在人口以万计数的时候每5年普查一次。美国宪法规定每10年进行一次人口普查，因为随着国家人口越来越多，只能以百万计数了。但是到19世纪为止，即使这样不频繁的人口普查依然很困难，因为数据变化的速度超过了人口普查局统计分析的能力。

    穿孔卡片的美国人口普查

    美国在1880年进行的人口普查，耗时8年才完成数据汇总。因此，他们获得的很多数据都是过时的。1890年进行的人口普查，预计要花费13年的时间来汇总数据。即使不考虑这种情况违反了宪法规定，它也是很荒谬的。然而，因为税收分摊和国会代表人数确定都是建立在人口的基础上的，所以必须要得到正确的数据，而且必须是及时的数据。

    美国人口普查局面临的问题与当代商人和科学家遇到的问题很相似。很明显，当他们被数据淹没的时候，已有的数据处理工具已经难以应付了，所以就需要有更多的新技术。

    后来，美国人口普查局就和当时的美国发明家赫尔曼·霍尔瑞斯（herman hollerith)签订了一个协议，用他的穿孔卡片制表机来完成1890年的人口普查。

    经过大量的努力，霍尔瑞斯成功地在1年时间内完成了人口普查的数据汇总工作。这简直就是一个奇迹，它标志着自动处理数据的开端，也为后来ibm公司的成立奠定了基础。但是，将其作为收集处理大数据的方法依然过于昂贵。毕竟，每个美国人都必须填一张可制成穿孔卡片的表格，然后再进行统计。这么麻烦的情况下，很难想象如果不足十年就要进行一次人口普查应该怎么办。但是，对于一个跨越式发展的国家而言，十年一次的人口普查的滞后『性』已经让普查失去了大部分意义。

    这就是问题所在，是利用所有的数据还是仅仅采用一部分呢？最明智的自然是得到有关被分析事物的所有数据，但是当数量无比庞大时，这又不太现实。那如何选择样本呢？有人提出有目的地选择最具代表『性』的样本是最恰当的方法。1934年，波兰统计学家耶日·奈曼（jerzy neyman）指出，这只会导致更多更大的漏洞。事实证明，问题的关键是选择样本时的随机『性』。

    统计学家们证明：采样分析的精确『性』随着采样随机『性』的增加而大幅提高，但与样本数量的增加关系不大。虽然听起来很不可思议，但事实上，一个对1100人进行的关于“是否”问题的抽样调查有着很高的精确『性』，精确度甚至超过了对所有人进行调查时的97%。这是真的，不管是调查10万人还是1亿人，20次调查里有19都是这样。为什么会这样？原因很复杂，但是有一个比较简单的解释就是，当样本数量达到了某个值之后，我们从新个体身上得到的信息会越来越少，就如同经济学中的边际效应递减一样。

    认为样本选择的随机『性』比样本数量更重要，这种观点是非常有见地的。这种观点为我们开辟了一条收集信息的新道路。通过收集随机样本，我们可以用较少的花费做出高精准度的推断。因此，『政府』每年都可以用随机采样的方法进行小规模的人口普查，而不是只能每十年进行一次。事实上，『政府』也这样做了。例如，除了十年一次的人口大普查，美国人口普查局每年都会用随机采样的方法对经济和人口进行200多次小规模的调查。当收集和分析数据都不容易时，随机采样就成为应对信息过量的办法。

    很快，随机采样就不仅应用于公共部门和人口普查了。在商业领域，随机采样被用来监管商品质量。这使得监管商品质量和提升商品品质变得更容易，花费也更少。以前，全面的质量监管要求对生产出来的每个产品进行检查，而现在只需从一批商品中随机抽取部分样品进行检查就可以了。本质上来说，随机采样让大数据问题变得更加切实可行。同理，它将客户调查引进了零售行业，将焦点讨论引进了政治界，也将许多人文问题变成了社会科学问题。

    随机采样取得了巨大的成功，成为现代社会、现代测量领域的主心骨。但这只是一条捷径，是在不可收集和分析全部数据的情况下的选择，它本身存在许多固有的缺陷。它的成功依赖于采样的绝对随机『性』，但是实现采样的随机『性』非常困难。一旦采样过程中存在任何偏见，分析结果就会相去甚远。

    最近，以固定电话用户为基础进行投票民调就面临了这样的问题，采样缺乏随机『性』，因为没有考虑到只使用移动电话的用户——这些用户一般更年轻和更热爱自由。没有考虑到这些用户，自然就得不到正确的预测。2008年在奥巴马与麦凯恩之间进行的美国总统大选中，盖洛普咨询公司、皮尤研究中心（pew）、美国广播公司和《华盛顿邮报》这些主要的民调组织都发现，如果他们不把移动用户考虑进来，民意测试结果就会出现三个点的偏差，而一旦考虑进来，偏差就只有一个点。鉴于这次大选的票数差距极其微弱，这已经是非常大的偏差了。

    更糟糕的是，随机采样不适合考察子类别的情况。因为一旦继续细分，随机采样结果的错误率会大大增加。这很容易理解。倘若你有一份随机采样的调查结果，是关于1000个人在下一次竞选中的投票意向。如果采样时足够随机，这份调查的结果就有可能在3%的误差范围内显示全民的意向。但是如果这个3%左右的误差本来就是不确定的，却又把这个调查结果根据『性』别、地域和收入进行细分，结果是不是越来越不准确呢？用这些细分过后的结果来表现全民的意愿，是否合适呢？

    你设想一下，一个对1000个人进行的调查，如果要细分到“东北部的富裕女『性』”，调查的人数就远远少于1000人了。即使是完全随机的调查，倘若只用了几十个人来预测整个东北部富裕女『性』选民的意愿，还是不可能得到精确结果啊！而且，一旦采样过程中存在任何偏见，在细分领域所做的预测就会大错特错。

    因此，当人们想了解更深层次的细分领域的情况时，随机采样的方法就不可取了。在宏观领域起作用的方法在微观领域失去了作用。随机采样就像是模拟照片打印，远看很不错，但是一旦聚焦某个点，就会变得模糊不清。

    随机采样也需要严密的安排和执行。人们只能从采样数据中得出事先设计好的问题的结果——千万不要奢求采样的数据还能回答你突然意识到的问题。所以虽说随机采样是一条捷径，但它也只是一条捷径。随机采样方法并不适用于一切情况，因为这种调查结果缺乏延展『性』，即调查得出的数据不可以重新分析以实现计划之外的目的。

    我们来看一下dna分析。由于技术成本大幅下跌以及在医学方面的广阔前景，个人基因排序成为了一门新兴产业。2012年，基因组解码的价格跌破1000美元，这也是非正式的行业平均水平。从2007年起，硅谷的新兴科技公司23andme就开始分析人类基因，价格仅为几百美元。这可以揭示出人类遗传密码中一些会导致其对某些疾病抵抗力差的特征，如『乳』腺癌和心脏病。23andme希望能通过整合顾客的dna和健康信息，了解到用其他方式不能获取的新信息。

    公司对某人的一小部分dna进行排序，标注出几十个特定的基因缺陷。这只是此人整个基因密码的样本，还有几十亿个基因碱基对未排序。最后，23andme只能回答它们标注过的基因组表现出来的问题。发现新标注时，此人的dna必须重新排序，更准确地说，是相关的部分必须重新排列。只研究样本而不是整体，有利有弊：能更快更容易地发现问题，但不能回答事先未考虑到的问题。

    大数据与乔布斯的癌症治疗

    苹果公司的传奇总裁史蒂夫·乔布斯在与癌症斗争的过程中采用了不同的方式，成为世界上第一个对自身所有dna和肿瘤dna进行排序的人。为此，他支付了高达几十万美元的费用，这是23andme报价的几百倍之多。所以，他得到的不是一个只有一系列标记的样本，他得到了包括整个基因密码的数据文档。

    对于一个普通的癌症患者，医生只能期望她的dna排列同试验中使用的样本足够相似。但是，史蒂夫·乔布斯的医生们能够基于乔布斯的特定基因组成，按所需效果用『药』。如果癌症病变导致『药』物失效，医生可以及时更换另一种『药』，也就是乔布斯所说的，“从一片睡莲叶跳到另一片上。”乔布斯开玩笑说：“我要么是第一个通过这种方式战胜癌症的人，要么就是最后一个因为这种方式死于癌症的人。”虽然他的愿望都没有实现，但是这种获得所有数据而不仅是样本的方法还是将他的生命延长了好几年。

    全数据模式，样本=总体

    在信息处理能力受限的时代，世界需要数据分析，却缺少用来分析所收集数据的工具，因此随机采样应运而生，它也可以被视为那个时代的产物。如今，计算和制表不再像过去一样困难。感应器、手机导航、网站点击和twitter被动地收集了大量数据，而计算机可以轻易地对这些数据进行处理。

    采样的目的就是用最少的数据得到最多的信息。当我们可以获得海量数据的时候，它就没有什么意义了。数据处理技术已经发生了翻天覆地的改变，但我们的方法和思维却没有跟上这种改变。

    然而，采样一直有一个被我们广泛承认却又总有意避开的缺陷，现在这个缺陷越来越难以忽视了。采样忽视了细节考察。虽然我们别无选择，只能利用采样分析法来进行考察，但是在很多领域，从收集部分数据到收集尽可能多的数据的转变已经发生了。如果可能的话，我们会收集所有的数据，即“样本=总体”。

    正如我们所看到的，“样本=总体”是指我们能对数据进行深度探讨，而采样几乎无法达到这样的效果。上面提到的有关采样的例子证明，用采样的方法分析整个人口的情况，正确率可达97%。对于某些事物来说，3%的错误率是可以接受的。但是你无法得到一些微观细节的信息，甚至还会失去对某些特定子类别进行进一步研究的能力。正态分布是标准的。生活中真正有趣的事情经常藏匿在细节之中，而采样分析法却无法捕捉到这些细节。

    谷歌流感趋势预测并不是依赖于对随机样本的分析，而是分析了整个美国几十亿条互联网检索记录。分析整个数据库，而不是对一个样本进行分析，能够提高微观层面分析的准确『性』，甚至能够推测出某个特定城市的流感状况，而不只是一个州或是整个国家的情况。farecast的初始系统使用的样本包含12000个数据，所以取得了不错的预测结果。但是随着奥伦·埃齐奥尼不断添加更多的数据，预测的结果越来越准确。最终，farecast使用了每一条航线整整一年的价格数据来进行预测。埃齐奥尼说：“这只是一个暂时『性』的数据，随着你收集的数据越来越多，你的预测结果会越来越准确。”

    所以，我们现在经常会放弃样本分析这条捷径，选择收集全面而完整的数据。我们需要足够的数据处理和存储能力，也需要最先进的分析技术。同时，简单廉价的数据收集方法也很重要。过去，这些问题中的任何一个都很棘手。在一个资源有限的时代，要解决这些问题需要付出很高的代价。但是现在，解决这些难题已经变得简单容易得多。曾经只有大公司才能做到的事情，现在绝大部分的公司都可以做到了。

    通过使用所有的数据，我们可以发现如若不然则将会在大量数据中淹没掉的情况。例如，信用卡诈骗是通过观察异常情况来识别的，只有掌握了所有的数据才能做到这一点。在这种情况下，异常值是最有用的信息，你可以把它与正常交易情况进行对比。这是一个大数据问题。而且，因为交易是即时的，所以你的数据分析也应该是即时的。

    然而，使用所有的数据并不代表这是一项艰巨的任务。大数据中的“大”不是绝对意义上的大，虽然在大多数情况下是这个意思。谷歌流感趋势预测建立在数亿的数学模型上，而它们又建立在数十亿数据节点的基础之上。完整的人体基因组有约30亿个碱基对。但这只是单纯的数据节点的绝对数量，并不代表它们就是大数据。大数据是指不用随机分析法这样的捷径，而采用所有数据的方法。谷歌流感趋势和乔布斯的医生们采取的就是大数据的方法。

    日本国民体育运动“相扑”中非法『操』纵比赛结果的发现，就恰到好处地说明了使用“样本=总体”这种全数据模式的重要『性』。消极比赛一直被极力禁止，备受谴责，很多运动员深受困扰。芝加哥大学的一位很有前途的经济学家斯蒂夫·列维特（steven levitt），在《美国经济评论》上发表了一篇研究论文，其中提到了一种发现这个情况的方法：查看运动员过去所有的比赛资料。他的畅销书《魔鬼经济学》（freakonomics）中也提到了这个观点，他认为检查所有的数据是非常有价值的。

    列维特和他的同事马克·达根（mark duggan）使用了11年中超过64000场摔跤比赛的记录，来寻找异常『性』。他们获得了重大的发现。非法『操』纵比赛结果的情况确实时有发生，但是不会出现在大家很关注的比赛上。冠军赛也有可能被『操』纵，但是数据显示消极比赛主要还是出现在不太被关注的联赛的后几场中。这时基本上没有什么风险，因为选手根本就没有获奖的希望。

    但是相扑比赛的一个比较特殊的地方是，选手需要在15场联赛中的大部分场次取得胜利才能保持排名和收入。这样一来就会出现利益不对称的问题。当一个7胜7负的摔跤手碰到一个8胜6负的对手时，比赛结果对第一个选手来说极其重要，对他的对手则没有那么重要。列维特和达根发现，在这样的情况下，需要赢的那个选手很可能会赢。这看起来像是对手送的“礼物”，因为在联系紧密的相扑界，帮别人一把就是给自己留了一条后路。

    有没有可能是要赢的决心帮助这个选手获胜呢？答案是，有可能。但是数据显示的情况是，需要赢的选手的求胜心也只是比平常高了25%。所以，把胜利完全归功于求胜心是不妥当的。对数据进行进一步分析可能会发现，与他们在前三四次比赛中的表现相比，当他们再相遇时，上次失利的一方要拥有比对方多3~4倍的胜率。

    这个情况是显而易见的。但是如果采用随机采样分析法，就无法发现这个情况。而大数据分析通过使用所有比赛的极大数据捕捉到了这个情况。这就像捕鱼一样，开始时你不知道是否能捕到鱼，也不知道会捕到什么鱼。

    一个数据库并不需要有以太字节（一般记做tb，等于2的40次方字节）计的数据。在这个相扑案例中，整个数据库包含的字节量还不如一张普通的数码照片包含得多。但是大数据分析法不只关注一个随机的样本。这里的“大”取的是相对意义而不是绝对意义，也就是说这是相对所有数据来说的。

    很长一段时间内，随机采样都是一条好的捷径，它使得数字时代之前的大量数据分析变得可能。但就像把一张数码照片或者一首数码歌曲截取成多个小文件似的，在采样分析的时候，很多信息都无法得到。拥有全部或几乎全部的数据，我们就能够从不同的角度，更细致地观察研究数据的方方面面。

    我们可以用lytro相机来打一个恰当的比方。lytro相机具有革新『性』的，因为它把大数据运用到了基本的摄影中。与传统相机只可以记录一束光不同，lytro相机可以记录整个光场里所有的光，达到1100万之多。具体生成什么样的照片则可以在拍摄之后再根据需要决定。用户没必要在一开始就聚焦，因为该相机可以捕捉到所有的数据，所以之后可以选择聚焦图像中的任一点。整个光场的光束都被记录了，也就是收集了所有的数据，“样本=总体”。因此，与普通照片相比，这些照片就更具“循环『性』”。如果使用普通相机，摄影师就必须在拍照之前决定好聚焦点。

    同理，因为大数据是建立在掌握所有数据，至少是尽可能多的数据的基础上的，所以我们就可以正确地考察细节并进行新的分析。在任何细微的层面，我们都可以用大数据去论证新的假设。是大数据让我们发现了相扑中的非法『操』纵比赛结果、流感的传播区域和对抗癌症需要针对的那部分dna。它让我们能清楚分析微观层面的情况。

    当然，有些时候，我们还是可以使用样本分析法，毕竟我们仍然活在一个资源有限的时代。但是更多时候，利用手中掌握的所有数据成为了最好也是可行的选择。

    社会科学是被“样本=总体”撼动得最厉害的学科。随着大数据分析取代了样本分析，社会科学不再单纯依赖于分析经验数据。这门学科过去曾非常依赖样本分析、研究和调查问卷。当记录下来的是人们的平常状态，也就不用担心在做研究和调查问卷时存在的偏见了。现在，我们可以收集过去无法收集到的信息，不管是通过移动电话表现出的关系，还是通过twitter信息表现出的感情。更重要的是，我们现在也不再依赖抽样调查了。

    艾伯特·拉斯洛·巴拉巴西（albert lászlobarabási），和他的同事想研究人与人之间的互动。于是他们调查了四个月内所有的移动通信记录——当然是匿名的，这些记录是一个为全美五分之一人口提供服务的无线运营商提供的。这是第一次在全社会层面用接近于“样本=总体”的数据资料进行网络分析。通过观察数百万人的所有通信记录，我们可以产生也许通过任何其他方式都无法产生的新观点。

    有趣的是，与小规模的研究相比，这个团队发现，如果把一个在社区内有很多连接关系的人从社区关系网中剔除开来，这个关系网会变得没那么高效但却不会解体；但如果把一个与所在社区之外的很多人有着连接关系的人从这个关系网中剔除，整个关系网很快就会破碎成很多小块。这个研究结果非常重要也非常的出人意料。谁能想象一个在关系网内有着众多好友的人的重要『性』还不如一个只是与很多关系网外的人联系的人呢？这说明一般来说无论是一个集体还是一个社会，多样『性』是有额外价值的。这个结果促使我们重新审视一个人在社会关系网中的存在价值。

     

第一部分 大数据时代的思维变革 02 更杂：不是精确性，而是混杂性

    允许不精确

    在越来越多的情况下，使用所有可获取的数据变得更为可能，但为此也要付出一定的代价。数据量的大幅增加会造成结果的不准确，与此同时，一些错误的数据也会混进数据库。然而，重点是我们能够努力避免这些问题。我们从不认为这些问题是无法避免的，而且也正在学会接受它们。这就是由“小数据”到“大数据”的重要转变之一。

    对“小数据”而言，最基本、最重要的要求就是减少错误，保证质量。因为收集的信息量比较少，所以我们必须确保记录下来的数据尽量精确。无论是观察天体的位置还是观测显微镜下物体的大小，为了使结果更加准确，很多科学家都致力于优化测量的工具。在采样的时候，对精确度的要求就更高更苛刻了。因为收集信息的有限意味着细微的错误会被放大，甚至有可能影响整个结果的准确『性』。

    历史上很多时候，人们会把通过测量世界来征服世界视为最大的成就。事实上，对精确度的高要求始于13世纪中期的欧洲。那时候，天文学家和学者对时间、空间的研究采取了比以往更为精确的量化方式，用历史学家阿尔弗雷德·克罗斯比（alfred crosby）的话来说就是“测量现实”。

    我们研究一个现象，是因为我们相信我们能够理解它。后来，测量方法逐渐被运用到科学观察、解释方法中，体现为一种进行量化研究、记录，并呈现可重复结果的能力。罗德·凯文（lord kelvin）曾说过：“测量就是认知。”这已成为一条至理名言。培根也曾说过：“知识就是力量。”同时，很多数学家以及后来的精算师和会计师都发展了可以准确收集、记录和管理数据的方法。

    19世纪，科技率先发展起来的法国开发了一套能准确计量时间、空间单位的系统，并逐渐成为其他国家普遍采用的标准，这套系统还为后来国际公认的测量条约奠定了基础，成为测量时代的巅峰。仅半个世纪之后，20世纪20年代，量子力学的发现永远粉碎了“测量臻于至善”的幻梦。然而，在物理学这个小圈子以外的一些测量工程师和科学家仍沉湎在完美测量的梦中。随着理『性』学科，如数学和统计学逐渐影响到商业领域，商业界更加崇尚这种思想。

    然而，在不断涌现的新情况里，允许不精确的出现已经成为一个新的亮点，而非缺点。因为放松了容错的标准，人们掌握的数据也多了起来，还可以利用这些数据做更多新的事情。这样就不是大量数据优于少量数据那么简单了，而是大量数据创造了更好的结果。

    同时，我们需要与各种各样的混『乱』做斗争。混『乱』，简单地说就是随着数据的增加，错误率也会相应增加。所以，如果桥梁的压力数据量增加1000倍的话，其中的部分读数就可能是错误的，而且随着读数量的增加，错误率可能也会继续增加。在整合来源不同的各类信息的时候，因为它们通常不完全一致，所以也会加大混『乱』程度。例如，与服务器处理投诉时的数据进行比较，用语音识别系统识别某个呼叫中心接到的投诉会产生一个不太准确的结果，但也是有助于我们把握整个事情的大致情况的。

    混『乱』还可以指格式的不一致『性』，因为要达到格式一致，就需要在进行数据处理之前仔细地清洗数据，而这在大数据背景下很难做到。“大数据”专家帕堤尔（d.j. patil）指出，i.b.m.、t.j. watson labs、international business machines都可以用来指代ibm，甚至可能有成千上万种方法称呼ibm。当然，在萃取或处理数据的时候，混『乱』也会发生。因为在进行数据转化的时候，我们是在把它变成另外的事物。比如，我们在对twitter的信息进行情感分析来预测好莱坞票房的时候，就会出现一定的混『乱』。其实，混『乱』的起源和类型本来就是一团『乱』麻。

    假设你要测量一个葡萄园的温度，但是整个葡萄园只有一个温度测量仪，那你就必须确保这个测试仪是精确的而且能够一直工作。反过来，如果每100棵葡萄树就有一个测量仪，有些测试的数据可能会是错误的，也可能会更加混『乱』，但众多的读数合起来就可以提供一个更加准确的结果。因为这里面包含了更多的数据，而它提供的价值不仅能抵消掉错误数据造成的影响，还能提供更多的额外价值。

    现在想想增加读数频率的这个事情。如果每隔一分钟就测量一下温度，我们至少还能够保证测量结果是按照时间有序排列的。如果变成每分钟测量十次甚至百次的话，不仅读数可能出错，连时间先后都可能搞混掉。试想，如果信息在网络中流动，那么一条记录很可能在传输过程中被延迟，在其到达的时候已经没有意义了，甚至干脆在奔涌的信息洪流中彻底『迷』失。虽然我们得到的信息不再那么准确，但收集到的数量庞大的信息让我们放弃严格精确的选择变得更为划算。

    在第一个例子里，我们为了获得更广泛的数据而牺牲了精确『性』，也因此看到了很多如若不然无法被关注到的细节。在第二个例子里，我们为了高频率而放弃了精确『性』，结果观察到了一些本可能被错过的变化。虽然如果我们能够下足够多的工夫，这些错误是可以避免的，但在很多情况下，与致力于避免错误相比，对错误的包容会带给我们更多好处。

    为了扩大规模，我们接受适量错误的存在。正如技术咨询公司forrester所认为的，有时得到2加2约等于3.9的结果，也很不错了。当然，数据不可能完全错误，但为了了解大致的发展趋势，我们愿意对精确『性』做出一些让步。

    我们可以在大量数据对计算机其他领域进步的重要『性』上看到类似的变化。我们都知道，如摩尔定律所预测的，过去一段时间里计算机的数据处理能力得到了很大的提高。摩尔定律认为，每块芯片上晶体管的数量每两年就会翻一倍。这使得电脑运行更快速了，存储空间更大了。大家没有意识到的是，驱动各类系统的算法也进步了——美国总统科技顾问委员会的报告显示，在很多领域这些算法带来的进步还要胜过芯片的进步。然而，社会从“大数据”中所能得到的，并非来自运行更快的芯片或更好的算法，而是更多的数据。

    由于象棋的规则家喻户晓，且走子限制良多，在过去的几十年里，象棋算法的变化很小。计算机象棋程序总是步步为赢是由于对残局掌握得更好了，而之所以能做到这一点也只是因为往系统里加入了更多的数据。实际上，当棋盘上只剩下六枚棋子或更少的时候，这个残局得到了全面地分析，并且接下来所有可能的走法（样本=总体）都被制入了一个庞大的数据表格。这个数据表格如果不压缩的话，会有一太字节那么多。所以，计算机在这些重要的象棋残局中表现得完美无缺和不可战胜。

    大数据在多大程度上优于算法这个问题在自然语言处理上表现得很明显（这是关于计算机如何学习和领悟我们在日常生活中使用语言的学科方向）。在2000年的时候，微软研究中心的米歇尔·班科（michele banko）和埃里克·布里尔（eric bill）一直在寻求改进word程序中语法检查的方法。但是他们不能确定是努力改进现有的算法、研发新的方法，还是添加更加细腻精致的特点更有效。所以，在实施这些措施之前，他们决定往现有的算法中添加更多的数据，看看会有什么不同的变化。很多对计算机学习算法的研究都建立在百万字左右的语料库基础上。最后，他们决定往4种常见的算法中逐渐添加数据，先是一千万字，再到一亿字，最后到十亿。

    结果有点令人吃惊。他们发现，随着数据的增多，4种算法的表现都大幅提高了。

    当数据只有500万的时候，有一种简单的算法表现得很差，但数据达10亿的时候，它变成了表现最好的，准确率从原来的75%提高到了95%以上。与之相反地，在少量数据情况下运行得最好的算法，当加入更多的数据时，也会像其他的算法一样有所提高，但是却变成了在大量数据条件下运行得最不好的。它的准确率会从86%提高到94%。

    后来，班科和布里尔在他们发表的研究论文中写到，“如此一来，我们得重新衡量一下更多的人力物力是应该消耗在算法发展上还是在语料库发展上。”

    大数据的简单算法比小数据的复杂算法更有效

    所以，数据多比少好，更多数据比算法系统更智能还要重要。那么，混『乱』呢？在班科和布里尔开始研究数据几年后，微软的最大竞争对手，谷歌，也开始更大规模地对这些问题进行探讨。谷歌用的是上万亿的语料库，而不是十亿的。谷歌做这类研究不是因为语法检查，而是为了解决翻译这个更棘手的难题。

    20世纪40年代，电脑由真空管制成，要占据整个房间这么大的空间。而机器翻译也只是计算机开发人员的一个想法。在冷战时期，美国掌握了大量关于苏联的各种资料，但缺少翻译这些资料的人手。所以，计算机翻译也成了亟须解决的问题。

    最初，计算机研发人员打算将语法规则和双语词典结合在一起。1954年，ibm以计算机中的250个词语和六条语法规则为基础，将60个俄语词组翻译成了英语，结果振奋人心。ibm701通过穿孔卡片读取了“mipyeryedaye mmislyi posryedstvom ryechyi”这句话，并且将其译成了“我们通过语言来交流思想”。在庆祝这个成就的发布会上，一篇报道就有提到，这60句话翻译得很流畅。这个程序的指挥官利昂·多斯特尔特（leon dostert）表示，他相信“在三五年后，机器翻译将会变得很成熟”。

    事实证明，计算机翻译最初的成功误导了人们。1966年，一群机器翻译的研究人员意识到，翻译比他们想象的更困难，他们不得不承认他们的失败。机器翻译不能只是让电脑熟悉常用规则，还必须教会电脑处理特殊的语言情况。毕竟，翻译不仅仅只是记忆和复述，也涉及选词，而明确地教会电脑这些非常不现实。法语中的“bonjour”就一定是“早上好”吗？有没有可能是“日安”、“你好”或者“喂”？事实上都有可能——这需要视情况而定。

    在20世纪80年代后期，ibm的研发人员提出了一个新的想法。与单纯教给计算机语言规则和词汇相比，他们试图让计算机自己估算一个词或一个词组适合于用来翻译另一种语言中的一个词和词组的可能『性』，然后再决定某个词和词组在另一种语言中的对等词和词组。

    20世纪90年代，ibm的这个candide项目花费了大概十年的时间，将大约有300万句之多的加拿大议会资料译成了英语和法语并出版。由于是官方文件，翻译的标准就非常高。用那个时候的标准来看，数据量非常之庞大。统计机器学习从诞生之日起，就聪明地把翻译的挑战变成了一个数学问题，而这似乎很有效！计算机翻译在短时间内就提高了很多。然而，在这次飞跃之后，ibm公司尽管投入了很多资金，但取得的成效不大。最终，ibm公司停止了这个项目。

    无所不包的谷歌翻译系统

    2006年，谷歌公司也开始涉足机器翻译。这被当作实现“收集全世界的数据资源，并让人人都可享受这些资源”这个目标的一个步骤。谷歌翻译开始利用一个更大更繁杂的数据库，也就是全球的互联网，而不再只利用两种语言之间的文本翻译。

    谷歌翻译系统为了训练计算机，会吸收它能找到的所有翻译。它会从各种各样语言的公司网站上去寻找联合国和欧洲委员会这些国际组织发布的官方文件和报告的译本。它甚至会吸收速读项目中的书籍翻译。谷歌翻译部的负责人弗朗兹·奥齐（franz och）是机器翻译界的权威，他指出，“谷歌的翻译系统不会像candide一样只是仔细地翻译300万句话，它会掌握用不同语言翻译的质量参差不齐的数十亿页的文档。”不考虑翻译质量的话，上万亿的语料库就相当于950亿句英语。

    尽管其输入源很混『乱』，但较其他翻译系统而言，谷歌的翻译质量相对而言还是最好的，而且可翻译的内容更多。到2012年年中，谷歌数据库涵盖了60多种语言，甚至能够接受14种语言的语音输入，并有很流利的对等翻译。之所以能做到这些，是因为它将语言视为能够判别可能『性』的数据，而不是语言本身。如果要将印度语译成加泰罗尼亚语，谷歌就会把英语作为中介语言。因为在翻译的时候它能适当增减词汇，所以谷歌的翻译比其他系统的翻译灵活很多。

    谷歌的翻译之所以更好并不是因为它拥有一个更好的算法机制。和微软的班科和布里尔一样，这是因为谷歌翻译增加了很多各种各样的数据。从谷歌的例子来看，它之所以能比ibm的candide系统多利用成千上万的数据，是因为它接受了有错误的数据。2006年，谷歌发布的上万亿的语料库，就是来自于互联网的一些废弃内容。这就是“训练集”，可以正确地推算出英语词汇搭配在一起的可能『性』。

    20世纪60年代，拥有百万英语单词的语料库——布朗语料库算得上这个领域的开创者，而如今谷歌的这个语料库则是一个质的突破，后者使用庞大的数据库使得自然语言处理这一方向取得了飞跃式的发展。自然语言处理能力是语音识别系统和计算机翻译的基础。彼得·诺维格（peter norvig），谷歌公司人工智能方面的专家，和他的同事在一篇题为《数据的非理『性』效果》（the unreasonable effectiveness of data）的文章中写道，“大数据基础上的简单算法比小数据基础上的复杂算法更加有效。”诺维格和他同事就指出，混杂是关键。

    “从某种意义上，谷歌的语料库是布朗语料库的一个退步。因为谷歌语料库的内容来自于未经过滤的网页内容，所以会包含一些不完整的句子、拼写错误、语法错误以及其他各种错误。况且，它也没有详细的人工纠错后的注解。但是，谷歌语料库是布朗语料库的好几百万倍大，这样的优势完全压倒了缺点。”

    纷繁的数据越多越好

    传统的样本分析师们很难容忍错误数据的存在，因为他们一生都在研究如何防止和避免错误的出现。在收集样本的时候，统计学家会用一整套的策略来减少错误发生的概率。在结果公布之前，他们也会测试样本是否存在潜在的系统『性』偏差。这些策略包括根据协议或通过受过专门训练的专家来采集样本。但是，即使只是少量的数据，这些规避错误的策略实施起来还是耗费巨大。尤其是当我们收集所有数据的时候，这就行不通了。不仅是因为耗费巨大，还因为在大规模的基础上保持数据收集标准的一致『性』不太现实。就算是不让人们进行沟通，也不能解决这个问题。

    大数据时代要求我们重新审视精确『性』的优劣。如果将传统的思维模式运用于数字化、网络化的21世纪，就会错过重要的信息。执『迷』于精确『性』是信息缺乏时代和模拟时代的产物。在那个信息贫乏的时代，任意一个数据点的测量情况都对结果至关重要。所以，我们需要确保每个数据的精确『性』，才不会导致分析结果的偏差。

    混杂『性』，不是竭力避免，而是标准途径

    确切地说，在许多技术和社会领域，我们更倾向于纷繁混杂。我们来看看内容分类方面的情况。几个世纪以来，人们一直用分类法和索引法来帮助自己存储和检索数据资源。这样的分级系统通常都不完善——各位读者没有忘记图书馆卡片目录给你们带来的痛苦回忆吧？在“小数据”范围内，这些方法就很有效，但一旦把数据规模增加好几个数量级，这些预设一切都各就各位的系统就会崩溃。

    相片分享网站flickr在2011年拥有来自大概1亿用户的60亿张照片。根据预先设定好的分类来标注每张照片就没有意义了。难道真会有人为他的照片取名“像希特勒一样的猫”吗？

    恰恰相反，清楚的分类被更混『乱』却更灵活的机制所取代。这些机制才能适应改变着的世界。当我们上传照片到flickr网站的时候，我们会给照片添加标签。也就是说，我们会使用一组文本标签来编组和搜索这些资源。人们用自己的方式创造和使用标签，所以它是没有标准、没有预先设定的排列和分类，也没有我们必须遵守的类别的。任何人都可以输入新的标签，标签内容事实上就成为网络资源的分类标准。标签被广泛地应用于facebook、博客等社交网络上。因为它们的存在，互联网上的资源变得更加容易找到，特别是像图片、视频和音乐这些无法用关键词搜索的非文本类资源。

    当然，有时人们错标的标签会导致资源编组的不准确，这会让习惯了精确『性』的人们很痛苦。但是，我们用来编组照片集的混『乱』方法给我们带来了很多好处。比如，我们拥有了更加丰富的标签内容，同时能更深更广地获得各种照片。我们可以通过合并多个搜索标签来过滤我们需要寻找的照片，这在以前是无法完成的。我们添加标签时所固带的不准确『性』从某种意义上说明我们能够接受世界的纷繁复杂。这是对更加精确系统的一种对抗。这些精确的系统试图让我们接受一个世界贫乏而规整的惨相——假装世间万物都是整齐地排列的。而事实上现实是纷繁复杂的，天地间存在的事物也远远多于系统所设想的。

    互联网上最火的网址都表明，它们欣赏不精确而不会假装精确。当一个人在网站上见到一个facebook的“喜欢”按钮时，可以看到有多少其他人也在点击。当数量不多时，会显示像“63”这种精确的数字。当数量很大时，则只会显示近似值，比方说“4000”。这并不代表系统不知道正确的数据是多少，只是当数量规模变大的时候，确切的数量已经不那么重要了。另外，数据更新得非常快，甚至在刚刚显示出来的时候可能就已经过时了。所以，同样的原理适用于时间的显示。谷歌的gmail邮箱会确切标注在很短时间内收到的信件，比方说“11分钟之前”。但是，对于已经收到一段时间的信件，则会标注如“两个小时之前”这种不太确切的时间信息。

    2000年以来，商务智能和分析软件领域的技术供应商们一直承诺给客户“一个唯一真理”。执行官们用这个词组并没有讽刺的意思，现在也依然有技术供应商这样说。他们说这个词组的意思就是，每个使用该公司信息技术系统的人都能利用同样的数据资源，这样市场部和营销部的人员们就不需要再在会议开始前争论，到底是谁掌握了正确的客户和销售数据了。这个想法就是说，如果他们知道的数据是一致的，那么他们的利益也会更一致。

    但是，“一个唯一的真理”这种想法已经彻底被改变了。现在不但出现了一种新的认识，即“一个唯一的真理”的存在是不可能的，而且追求这个唯一的真理是对注意力的分散。要想获得大规模数据带来的好处，混『乱』应该是一种标准途径，而不应该是竭力避免的。

    我们甚至发现，不精确已经渗入了数据库设计这个最不能容忍错误的领域。传统的数据库引擎要求数据高度精确和准确排列。数据不是单纯地被存储，它往往被划分为包含“域”的记录，每个域都包含了特定种类和特定长度信息。比方说，某个数值域是7个数字长，一个1000万或者更大的数值就无法被记录。一个人想在某个记录手机号码的域中输入一串汉字是“不被允许”的。想要被允许也可以，需要改变数据库结构才可以。现在，我们依然在和电脑以及智能手机上的这些限制进行斗争，比如软件可能拒绝记录我们输入的数据。

    索引是事先就设定好了的，这也就限制了人们的搜索。增加一个新的索引往往既消耗时间，又惹人讨厌，因为需要改变底层的设计。传统的关系数据库是为数据稀缺的时代设计的，所以能够也需要仔细策划。在那个时代，人们遭遇到的问题无比清晰，所以数据库被设计用来有效地回答这些问题，

    但是，这种数据存储和分析的方法越来越和现实相冲突。我们现在拥有各种各样、参差不齐的海量数据，很少有数据完全符合预先设定的数据种类。而且，我们想要的数据回答的问题，也只有在我们收集和处理数据的过程中才会知道。

     

第一部分 大数据时代的思维变革 03 更好：不是因果关系，而是相互关系

    “是什么”，而不是“为什么”

    在小数据时代，相关关系分析和因果分析都不容易，都耗费巨大，都要从建立假设开始。然后我们会进行实验——这个假设要么被证实要么被推翻。但由于两者都始于假设，这些分析就都有受偏见影响的可能，而且极易导致错误。与此同时，用来做相关关系分析的数据很难得到，收集这些数据时也耗资巨大。现今，可用的数据如此之多，也就不存在这些难题了。

    当然，还有一种不同的情况也逐渐受到了人们的重视。在小数据时代，由于计算机能力的不足，大部分相关关系分析仅限于寻求线『性』关系。这个情况随着数据的增加肯定会发生改变。事实上，实际情况远比我们所想象的要复杂。经过复杂的分析，我们能够发现数据的“非线『性』关系”。

    当相关关系变得更复杂时，一切就更混『乱』了。比如，各地麻疹疫苗接种率的差别与人们在医疗保健上的花费似乎有关联。但是，最近哈佛与麻省理工的联合研究小组发现，这种关联不是简单的线『性』关系，而是一个复杂的曲线图。和预期相同的是，随着人们在医疗上花费的增多，麻疹疫苗接种率的差别会变小；但令人惊讶的是，当增加到一定程度时，这种差别又会变大。发现这种关系对公共卫生官员来说非常重要，但是普通的线『性』关系分析师是无法捕捉到这个重要信息的。

    如今，专家们正在研发能发现并对比分析非线『性』关系的必要技术工具。一系列飞速发展的新技术和新软件也从多方面提高了相关关系分析工具发现非因果关系的能力，这就好比立体派画家同时从多个角度来表现女『性』脸庞的手法。

    网络分析行业的出现就是一个最明显的例子。多亏了它，让描绘、测量、计算各节点之间的关系变成了可能，我们可以从facebook上认识更多的朋友，还可以知道法庭上的一些判决的先例，以及谁给谁打了电话。总之，这些工具为回答非因果关系及经验『性』的问题提供了新的途径。

    在大数据时代，这些新的分析工具和思路为我们提供了一系列新的视野和有用的预测，我们看到了很多以前不曾注意到的联系，还掌握了以前无法理解的复杂技术和社会动态。但最重要的是，通过去探求“是什么”而不是“为什么”，相关关系帮助我们更好地了解了这个世界。

    这听起来似乎有点违背常理。毕竟，人们都希望通过因果关系来了解这个世界。我们也相信，只要仔细观察，就会发现万事万物皆有因缘。了解事情的起因难道不是我们最大的愿望吗？

    在哲学界，关于因果关系是否存在的争论已经持续了几个世纪。毕竟，如果凡事皆有因果的话，那么我们就没有决定任何事的自由了。如果说我们做的每一个决定或者每一个想法都是其他事情的结果，而这个结果又是由其他原因导致的，以此循环往复，那么就不存在人的自由意志这一说了——所有的生命轨迹都只是受因果关系的控制了。因此，对于因果关系在世间所扮演的角『色』，哲学家们争论不休，有时他们认为，这是与自由意志相对立的。当然，关于理论的争辩并不是我们要研究的重点。

    首先，我们的直接愿望就是了解因果关系。即使无因果关系存在，我们也还是会假定其存在。研究证明，这只是我们的认知方式，与每个人的文化背景、生长环境与教育水平是无关的。当我们看到两件事情接连发生的时候，我们会习惯『性』地从因果关系的角度来看待它们。看看下面的三句话：“弗雷德的父母迟到了；供应商快到了；弗雷德生气了。”

    我们读到这里时，可能立马就会想到弗雷德生气并不是因为供应商快到了，而是他父母迟到的缘故。实际上，我们也不知道到底是什么情况。即便如此，我们还是不禁认为这些假设的因果关系是成立的。

    普林斯顿大学心理学专家，同时也是2002年诺贝尔经济学奖得主丹尼尔·卡尼曼（daniel kahneman）就是用这个例子证明了人有两种思维模式。第一种是不费力的快速思维，通过这种思维方式几秒钟就能得到出结果；另一种是比较费力的慢『性』思维，对于特定的问题，就是需要考虑到位。

    快速思维模式使人们用因果联系来看待周围的一切，即使这种关系并不存在。这是我们对已有的知识和信仰的执著。在古代，这种快速思维模式是很有用的，它能帮助我们在信息量缺乏却必须快速做出决定的危险情况下化险为夷。但是，通常这种因果关系都是并不存在的。

    卡尼曼指出，平时生活中，由于惰『性』，我们很少慢条斯理地思考问题。所以快速思维模式就占据了上风。因此，我们会经常臆想出一些因果关系，最终导致了对世界的错误理解。

    父母经常告诉孩子，天冷时不戴帽子和手套就会感冒。然而，事实上，感冒和穿戴之间却没有直接的联系。有时，我们在某个餐馆用餐生病了的话，我们就会自然而然地觉得这是餐馆食物的问题，以后可能就不再去这家餐馆了。事实上，我们肚子痛也许是因为其他的传染途径，比如和患者握过手之类的。然而，我们的快速思维模式使我们直接将其归于任何我们能在第一时间想起来的因果关系，因此，这经常导致我们做出错误的决定。

    与常识相反，经常凭借直觉而来的因果关系并没有帮助我们加深对这个世界的理解。很多时候，这种认知捷径只是给了我们一种自己已经理解的错觉，但实际上，我们因此完全陷入了理解误区之中。就像采样是我们无法处理数据时的捷径一样，这种找因果关系的方法也是我们大脑用来避免辛苦思考的捷径。

    在小数据时代，很难证明由直觉而来的因果联系是错误的。现在，情况不一样了。将来，大数据之间的相关关系，将经常会用来证明直觉的因果联系是错误的。最终也能表明，统计关系也不蕴含多少真实的因果关系。总之，我们的快速思维模式将会遭受各种各样的现实考验。

    令人欣喜的是，为了更好地了解世界，我们会因此更加努力地思考。但是，即使是我们用来发现因果关系的第二种思维方式——慢『性』思维，也将因为大数据之间的相关关系迎来大的改变。

    日常生活中，我们习惯『性』地用因果关系来考虑事情，所以会认为，因果联系是浅显易寻的。但事实却并非如此。与相关关系不一样，即使用数学这种比较直接的方式，因果联系也很难被轻易证明。我们也不能用标准的等式将因果关系表达清楚。因此，即使我们慢慢思考，想要发现因果关系也是很困难的。因为我们已经习惯了信息的匮乏，故此亦习惯了在少量数据的基础上进行推理思考，即使大部分时候很多因素都会削弱特定的因果关系。

    就拿狂犬疫苗这个例子来说，1885年7月6日，法国化学家路易·巴斯德（louis pasteur）接诊了一个9岁的小孩约瑟夫·梅斯特（joseph meister），他被带有狂犬病毒的狗咬了。那时，巴斯德刚刚研发出狂犬疫苗，也实验验证过效果了。梅斯特的父母恳求巴斯德给他们的儿子注『射』一针。巴斯德做了，梅斯特活了下来。发布会上，巴斯德因为把一个小男孩从死神手中救出而大受褒奖。

    但真的是因为他吗？事实证明，人被狂犬病狗咬后患上狂犬病的概率只有七分之一。即使巴斯德的疫苗有效，这也只适用于七分之一的案例中。无论如何，就算没有狂犬疫苗，这个小男孩活下来的概率还是有85%。

    在这个例子中，大家都认为是注『射』疫苗救了梅斯特一命。但这里却有两个因果关系值得商榷。第一个是疫苗和狂犬病毒之间的因果关系，第二个就是被带有狂犬病毒的狗咬和患狂犬病之间的因果关系。即便是说疫苗能够医好狂犬病，第二个因果关系也只适用于极少数情况。

    不过，科学家已经克服了用实验来证明因果关系的难题。实验是通过是否有诱因这两种情况，分别来观察所产生的结果是不是和真实情况相符，如果相符就说明确实存在因果关系。这个衡量假说的验证情况控制得越严格，你就会发现因果关系越有可能是真实存在的。

    因此，与相关关系一样，因果关系被完全证实的可能『性』几乎是没有的，我们只能说，某两者之间很有可能存在因果关系。但两者之间又有不同，证明因果关系的实验要么不切实际，要么违背社会伦理道德。比方说，我们怎么从5亿词条中找出和流感传播最相关的呢？我们难道真能为了找出被咬和患病之间的因果关系而置成百上千的病人的生命于不顾吗？因为实验会要求把部分病人当成未被咬的“控制组”成员来对待，但是就算给这些病人打了疫苗，我们又能保证万无一失吗？而且就算这些实验可以『操』作，『操』作成本也非常的昂贵。

    不像因果关系，证明相关关系的实验耗资少，费时也少。与之相比，分析相关关系，我们既有数学方法，也有统计学方法，同时，数学工具也能帮助我们准确地找出相关关系。

    相关关系分析本身意义重大，同时它也为研究因果关系奠定了基础。通过找出可能相关的事物，我们可以在此基础上进行进一步的因果关系分析，如果存在因果关系的话，我们再进一步找出原因。这种便捷的机制通过严格的实验降低了因果分析的成本。我们也可以从相互联系中找出一些重要的变量，这些变量可以用到验证因果关系的实验中去。

    可是，我们必须非常认真。相关关系很有用，不仅仅是因为它能为我们提供新的视角，而且提供的视角都很清晰。而我们一旦把因果关系考虑进来，这些视角就有可能被蒙蔽掉。

    例如，kaggle，一家为所有人提供数据挖掘竞赛平台的公司，举办了关于二手车的质量竞赛。二手车经销商将二手车数据提供给参加比赛的统计学家，统计学家们用这些数据建立一个算法系统来预测经销商拍卖的哪些车有可能出现问题。相关关系分析表明，橙『色』的车有质量问题的可能『性』只有其他车的一半。

    当我们读到这里的时候，不禁也会思考其中的原因。难道是因为橙『色』车的车主更爱车，所以车被保护得更好吗？或是这种颜『色』的车子在制造方面更精良些吗？还是因为橙『色』车更显眼、出车祸的概率更小，所以转手的时候，各方面的『性』能保持得更好？

    马上，我们就陷入了各种各样谜一样的假设中。若要找出相关关系，我们可以用数学方法，但如果是因果关系的话，这却是行不通的。所以，我们没必要一定要找出相关关系背后的原因，当我们知道了“是什么”的时候，“为什么”其实没那么重要了，否则就会催生一些滑稽的想法。比如说上面提到的例子里，我们是不是应该建议车主把车漆成橙『色』呢？毕竟，这样就说明车子的质量更过硬啊！

    考虑到这些，如果把以确凿数据为基础的相关关系和通过快速思维构想出的因果关系相比的话，前者就更具有说服力。但在越来越多的情况下，快速清晰的相关关系分析甚至比慢速的因果分析更有用和更有效。慢速的因果分析集中体现为通过严格控制的实验来验证的因果关系，而这必然是非常耗时耗力的。

    近年来，科学家一直在试图减少这些实验的花费，比如说，通过巧妙地结合相似的调查，做成“类似实验”。这样一来，因果关系的调查成本就降低了，但还是很难与相关关系体现的优越『性』相抗衡。还有，正如我们之前提到的，在专家进行因果关系的调查时，相关关系分析本来就会起到帮助的作用。

    因果关系还是有用的，但是它将不再被看成是意义来源的基础。在大数据时代，即使很多情况下，我们依然指望用因果关系来说明我们所发现的相互关系，但是，我们知道因果关系只是一种特殊的相关关系。相反，大数据推动了相关关系分析。相关关系分析通常情况下能取代因果关系起作用，即使不可取代的情况下，它也能知道因果关系起作用。曼哈顿沙井盖（即下水道的修检口）的爆炸就是一个很好的例子。

    大数据，改变人类探索世界的方法

    在小数据时代，我们会假想世界是怎样运作的，然后通过收集和分析数据来验证这种假想。在不久的将来，我们会在大数据的指导下探索世界，不再受限于各种假想。我们的研究始于数据，也因为数据我们发现了以前不曾发现的联系。

    假想通常来自自然理论或社会科学，它们也是帮助我们解释和预测周遭世界的基础。随着由假想时代到数据时代的过渡，我们也很可能认为我们不再需要理论了。

    2008年，《连线》杂志主编克里斯·安德森（chris anderson）就指出：“数据爆炸使得科学的研究方法都落伍了。”后来，他又在《拍字节时代》（the petabute age）的封面故事中讲到，大量的数据从某种程度上意味着“理论的终结”。安德森也表示，用一系列的因果关系来验证各种猜想的传统研究范式已经不实用了，如今它已经被无需理论指导的纯粹的相关关系研究所取代。

    为了支撑自己的观点，安德森阐述了量子物理学已变成一门纯理论学科的原因，就是因为实验服装、耗费多且不可行。他潜在的观点就是，量子物理学的理论已经脱离实际。他提到了谷歌的搜索引擎和基因排序工程，指出：“现在已经是一个有海量数据的时代，应用数学已经取代了其他的所有学科工具。而且只要数据足够，就能说明问题。如果你有一拍字节的数据，只要掌握了这些数据之间的相关关系，一切就都迎刃而解了。”

    这篇文章引发了激烈的争论，虽然安德森本人很快就意识到自己的言辞过于激烈了，但是他的观点确实值得深思。安德森的核心思想是，直到目前为止，我们一直都是把理论应用到实践中来分析和理解世界，而如今处在大数据时代，我们不再需要理论了，只要关注数据就足够了。这就意味着所有的普遍规则都不重要了，比方说世界的运作，人类的行为，顾客买什么，东西什么时候会坏等。如今，重要的就是数据分析，它可以揭示一切问题。

    大数据是在理论的基础上形成的。比方说，大数据分析就用到了统计和数学理论，有时候也会用到计算机科学理论。是的，这不是关于像地心引力这样特定现象的产生原因的理论，但是无论如果这依然是理论。而且如我们所见，建立在这些理论上的大数据分析模式是实现大数据预测能力的重要因素。事实上，就是因为不受限于传统的思维模式和特定领域里隐含的固有偏见，大数据才能为我们提供如此多新的深刻洞见。

    首先就是关于我们怎么收集数据。我们会不会仅仅看数据收集的方便程度来决定呢？或者看数据收集的成本？我们做这些决定的时候就被理论所影响着，而就如达纳·博尹德（danah boyd）和凯特·克劳福德（kate crawford）说的，我们的选择一定程度上决定了结果。毕竟，谷歌是用检索词来预测流感而不是鞋码。同样，我们在分析数据的时候，也依赖于理论来选择我们使用的工具。最后，我们解读研究结果的时候同样会使用理论。大数据时代绝对不是一个理论消亡的时代，相反地，理论贯穿于大数据分析的方方面面。

    作为第一提出问题的人，安德森应该获得掌声——尽管他的答案不怎么样！大数据绝不会叫嚣“理论已死”，但它毫无疑问会从根本上改变我们理解世界的方式。很多旧有的习惯将被颠覆，很多旧有的制度将面临挑战。

    大数据时代将要释放出的巨大价值使得我们选择大数据的理念和方法不再是一种权衡，而是通往未来的必然改变。但是在我们到达目的地之前，我们有必要了解怎样才能到达。高科技行业里的很多人认为是依靠新的工具，从高速芯片到高效软件等。当然，这可以理解为因为他们自己是工具创造者。这些问题固然重要，但不是我们需要考虑的问题。大数据趁势的深层原因，就是海量数据的存在以及越来越多的事物是以数据形式存在的，这也是我们下一章要谈论的内容。

     

第二部分 大数据时代的商业变革 04 数据化：一切皆可“量化”

    量化一切，数据化的核心

    记录信息的能力是原始社会和先进社会的分界线之一。早期文明最古老的抽象工具就是基础的计算以及长度和重量的计量。公元前3000年，信息记录在印度河流域、埃及和美索不达米亚平原地区就有了很大的发展，而日常的计量方法也大有改善。美索不达米亚平原上书写的发展促使了一种记录生产和交易的精确方法的产生，这让早期文明能够计量并记载事实情况，并且为日后所用。计量和记录一起促成了数据的诞生，它们是数据化最早的根基。

    计量和记录能够再现人类活动。比如通过记录建筑物的建筑方式和原材料，我们就能再建同样的建筑，或进行实验『性』的『操』作，比如通过改变一些方式保存其他部分而建造出新的建筑物，然后再记录这些新建筑物。交易情况一旦得到记录，我们就可以知道一块地丰收时稻谷的产量是多少、需要上缴多少『政府』税收。计量和记录为预测和计划奠定了基础，虽然这建立在假定明年的收成和今年一样的基础上。有了记录，交易双方才会知道他们赊账的情况，而如果没有这些凭证的支持，欠债的一方则完全可以不用还钱。

    几百年来，计量从长度和重量不断扩展到了面积、体积和时间。公元前的最后一个千年，西方的计量方法已经基本准备就绪，但是还是有着比较严重的缺陷。早期文明的计量方法不太适合计算，哪怕是比较简单的计算。比如罗马数字的计算系统就不适合数字计算，因为它没有一个以10为底的记数制或者说是十进制，所以大数目的乘除就算是专家都不知道该怎么算，而简单的乘除对一般人来说也不容易。

    大约公元1世纪的时候，印度发明了一种自己的数字系统。它传播到了波斯，并在那里得到改善，而后传入阿拉伯国家，得到了极大的改进。这也就是今天使用的阿拉伯数字的前身。十字军东征给当地人民带来了彻头彻尾的灾难，但同时也把西欧文明带到了地中海东部，而其中最重要的引入就是阿拉伯数字。公元1000年，教皇西尔维斯特二世开始倡导使用阿拉伯数字。12世纪，介绍阿拉伯数字的书籍被翻译成拉丁文，传播到了整个欧洲地区。这也就开启了算术的腾飞。

    早在阿拉伯数字传播到欧洲之前，计数板的使用就已经改善了算术。计数板就是在光滑的托盘上放上代币来表示数量，人们通过移动代币到某个区域进行加减。但是，这种计数板有着严重的缺陷，即过大和过小的计算无法同时进行。最主要的缺陷还在于，这些计数板上的数字变化很快，不小心的碰撞或者是摆错一位都会导致完全错误的结果。而且，即便计数板勉强可以进行计算，它也不适合用来记录。因为一旦需要将数字记录在计数板以外的地方，就必须把计数板上的数字转化成罗马数字，这可就费时费力了。

    算术赋予了数据新的意义，因为它现在不但可以被记录还可以被分析和再利用。阿拉伯数字从12世纪开始在欧洲出现，而直到16世纪晚期才被广泛采用。到16世纪的时候，数学家们大肆鼓吹他们使用阿拉伯数字计算能比使用计数板快6倍。但最终让阿拉伯数字广为采用的还是复式记账法的出现，它也是数据化的一种工具。

    公元前3000年，会计手稿就出现了。但是，记账法在接下来的几百年里发展缓慢，基本上一直保持在记录某地的某个特定交易的阶段。记账人和他的雇主最关心的就是判断某个账户或者自己所从事的行业是否赚钱，而这正是当时的记账手法无法轻易做到的事情。到了14世纪，随着意大利的会计们开始使用两个账本记录交易明细，这种尴尬的境地开始发生改变。这种记账法的优势在于，人们只需要将借贷相加，就可进行制表并得知每个账户的盈亏情况。如此，数据骤然发声了，虽然仅限于读出盈亏情况。

    如今，复式记账法通常被看成是会计业和金融业不断发展的成果。事实上，在数据利用的推进过程中，它也是一个里程碑似的存在。它的出现实现了相关账户信息的“分门别类”记录。它建立在一系列记录数据的规则之上，也是最早的信息记录标准化的例子，使得会计们能够读懂彼此的账本。复式记账法可以使查询每个账户的盈亏情况变得简单容易。它会提供交易的记账线索，这样就更容易找到需要的数据。它的设计理念中包含了“纠错”的思想，这也是今天的技术人才们应该学习的。如果一个账本看着不对劲，我们可以查询另一个相对应的账本。

    但是，和阿拉伯数字一样，复式记账法也没有立即取得成功。直到200年之后，一个数学家和一个商业家族才让它大受欢迎，他们也改变了数据化的历史。

    这个数学家就是方济各会的修士路萨·帕西奥利（luca pacioli）。1494年，他出版了一本为普通读者和商人所写的数学教材。这本书大获成功，成为盛行一时的数学教科书。这是第一本全书都使用阿拉伯数字的书籍，因此也促进了阿拉伯数字在欧洲的传播。当然，这本书最大的贡献在于它对复式记账法的详尽论述。接下来的几十年间，这个论述复式记账法的部分被分别译成了6种语言，并且成为几个世纪的通用范本。

    而所谓的一个商业家族，就是指美第齐家族——威尼斯商人和艺术资助人。16世纪，这个家族能成为欧洲最有影响力的银行家族，很大一部分要归功于他们使用的一种高级数据记录方法——复式记账法。帕西奥利的著作和美第齐家族的成功奠定了复式记账法成为标准数据记录法的基础，也奠定了阿拉伯数字在此之后不可取代的地位。

    伴随着数据记录的发展，人类探索世界的想法一直在膨胀，我们渴望能更精准地记录时间、距离、地点、体积和重量，等等。到了19世纪，随着科学家们发明了新工具来测量和记录电流、气压、温度、声频之类的自然科学现象，科学已经离不开定量化了。那是一个一切事物都需要被测量、划分和记录的时代，人们理解自然的热情甚至高涨到通过分析测量人的颅骨来试图分析人的心智能力。好在，对颅相学这类伪科学的热情最终淡去了，但是人类对于量化一切的热情却始终没有减退。

    新工具和开放的思维促进了测量事物和记录数据的繁荣，而现代数据化就诞生于这片沃土之中。数据化的基础已经奠定完好，只是在模拟时代这依然是费时费力的。有时候似乎需要无穷无尽的热情和耐心，或者说，起码也要有奉献一生的准备，比如16世纪的第谷·布拉赫（tycho brahe）就夜夜细心观察天体运动。数据化在模拟时代成功的例子并不多，因为这需要很好的运气——一大串的偶然巧妙地结合在一起。中校莫里就很幸运，他因伤坐进了办公室，但是却在那里发现了珍贵的航海日志，可不是每个人都能这么幸运的。然而，数据化的实现有一点必不可少，那就是要从潜在的数据中挖掘出巨大的价值，然后揭示出新的深刻洞见。

    计算机的出现带来了数字测量和存储设备，这样就大大提高了数据化的效率。计算机也使得通过数学分析挖掘出数据更大的价值变成了可能。简而言之，数字化带来了数据化，但是数字化无法取代数据化。数字化是把模拟数据变成计算机可读的数据，和数据化有本质的不同。

    世间万物的数据化

    只要一点想象，万千事物就能转化为数据形式，并一直带给我们惊喜。ibm获得的“触感技术先导”专利与东京的越水重臣教授对『臀』部的研究工作具有相同理念。知识产权律师称那是一块触感灵敏的地板，就像一个巨大的智能手机屏幕。其潜在的用途十分广泛。它能分辨出放置其上的物品。它的基本用途就是适时地开灯和开门。然而更重要的是，它能通过一个人的体重、站姿和走路方式确认他的身份。它还能知道某人在摔倒之后是否一直没有站起来。有了它，零售商可以知道商店的人流量。当地板数据化了的时候，它能滋生无穷无尽的用途。

    其实没有听上去那么荒谬。“自我量化”是一项由一群健身『迷』、医学疯子以及技术狂人发起的运动，通过测量身体的每一个部位和生活中的每一件事来让生活更美好——或者至少用量化的方式来获得新知。目前，自我量化运动规模还很小，但正在日益壮大。

    随着智能手机和计算机技术的普及，对个人最重要的生活行为进行数据处理从未如现在这般容易。许多创业公司通过测量人们夜间的脑电波来试图找出他们的睡眠模式。zeo公司则早已制作出了世界上最大的睡眠活动数据库，揭示了男『性』与女『性』睡眠时快速眼动量的差异。asthmapolis公司将一个感应器绑定到哮喘病人佩戴的呼吸器上，通过gps定位，再汇总收集起来的位置数据，可以判断环境因素（如接近特定的农作物）对哮喘的影响。fitbit和jawbone公司让人们测量他们的体力活动和睡眠。basis公司用腕带来监测佩戴者的生命体征，包括其心率和皮肤电传导率，以此测试他们所承受的压力。2009年，苹果公司就申请了一项专利，通过音频耳塞收集关于血『液』氧合、心率和体温的数据。获取数据正变得比以往任何时候都简单而不受限制。

    数据化能帮助我们获取到更多关于人体运作方式的信息。挪威耶维克大学的研究人员和derawi biometrics公司联合为智能手机开发了一款应用程序，可以分析人走路时的步伐并将其作为手机解锁的安全系统。同时，佐治亚理工学院的罗伯特·德拉诺（robert delano）和布莱恩·派尔思（brian parise）开发了一款叫做itrem的应用程序，用手机内置的测震仪监测人身体的颤动，以应对帕金森和其他神经系统疾病。这个程序给医生和病人都带来了好处；它让患者避免了在医院做昂贵的体检，也让医学专家们能远程监控人们的疾病以及治疗效果。据东京的调查人员说，用智能手机测量震动虽然没有三轴测震仪这种专门的医疗器械那么精确，但也只差了一点，所以完全可以放心使用。这再一次证明，一点点的不精确比完全精确更有效。

    在大多数情况下，我们会采集信息并将之存储为数据形式再加以利用。几乎所有领域，任何事情都能这样处理。greengoose是一家创业公司，他们销售能放置在物品上的微型运动感应器，用它监测物品的使用次数。比如把它放置在一捆牙线、一个酒水壶或者一盒猫食上，就能数据化牙齿清洁、植物护理以及宠物喂养的信息。很多人对“物联网”有着宗教般的狂热，试图在一切生活中的事物中都植入芯片、传感器和通信模块。这个词听起来好像和互联网亲如姐妹，其实不过是一种典型的数据化手段罢了。

    我们正在进行一个重大的基础设施项目，它在某种程度上与我们过去所做的都不一样，无论是罗马的水渠还是启蒙运动时期的百科全书。它如此的新颖，而我们又深处其中；同时，又因为它是无形的，不像水渠中能触『摸』到的水，所以我们并未意识到它的存在，这个它，就是无处不在的数据化。像其他的基础设施那样，它会给社会带来根本『性』的变革。

    水渠让城市的发展成为可能，印刷机推进了启蒙运动，报纸为民族国家的兴起奠定了基础。但这些基础设施都侧重于流动——关于水、关于知识。电话和互联网也是如此。相比较而言，数据化代表着人类认识的一个根本『性』转变。有了大数据的帮助，我们不会再将世界看作是一连串我们认为或是自然或是社会现象的事件，我们会意识到本质上世界是由信息构成的。

    整整一个多世纪以来，物理学家们一直宣称情况应该是这样的——并非原子而是信息才是一切的本源。不可否认，这也许听上去无法理解。然而通过数据化，在很多情况下我们就能全面采集和计算有形物质和无形物质的存在，并对其进行处理。

    将世界看作信息，看作可以理解的数据的海洋，为我们提供了一个从未有过的审视现实的视角。它是一种可以渗透到所有生活领域的世界观。

    迟早有一天，数据化的影响会使水渠和报纸的影响微乎其微，同时，通过赋予人类数据化世间万物的工具，它也对印刷机和互联网的地位提出了挑战。可是目前，它最主要的用途还是在商业领域。大数据正被用来创造新型价值，这也是下一章的主题。

     

第二部分 大数据时代的商业变革 05 价值：“取之不尽，用之不竭”的数据创新

    给数据估值

    无论是向公众开放还是将其锁在公司的保险库中，数据的价值都难以衡量。来看看2012年5月18日星期五发生的事吧。这一天，28岁的facebook创始人马克·扎克伯格（mark zuckerberg）在位于美国加利福尼亚州门洛帕克市的公司总部，象征『性』地敲响了纳斯达克的开盘钟。这家宣称全球约每十人中就有一人是其用户的全球最大社交网络公司，开启了其作为上市公司的征程。

    和很多新科技股的第一个上市交易日一样，公司股价立即上涨了11%，翻倍增长甚至已经近在眼前。然而就在这一天，怪事发生了。facebook的股价开始下跌，期间纳斯达克的电脑因出现技术故障曾暂停交易，但仍然于事无补，情况甚至更加恶化。感到异常的股票承销商在摩根士丹利的带领下，不得不支撑股价，最终以略高于发行价收盘。

    上市的前一晚，银行对facebook的定价是每股38美元，总估值1040亿美元（也就是说，大约是波音公司、通用汽车和戴尔电脑的市值之和）。那么事实上facebook价值多少呢？在2011年供投资者评估公司的审核账目中，facebook公布的资产为66亿美元，包括计算机硬件、专利和其他实物价值。那么facebook公司数据库中存储的大量信息，其账面价值是多少呢？零。它根本没有被计入其中，尽管除了数据，facebook几乎一文不值。

    这令人匪夷所思。加特纳市场研究公司（gartner）的副总裁道格·莱尼（doug laney）研究了facebook在ipo前一段时间内的数据，估算出facebook在2009年至2011年间收集了2.1万亿条“获利信息”，比如用户的“喜好”、发布的信息和评论等。与其ipo估值相比，这意味着每条信息（将其视为一个离散数据点）都有约4美分的价值。也就是说，每一个facebook用户的价值约为100美元，因为他们是facebook所收集信息的提供者。

    那么，如何解释facebook根据会计准则计算出的价值（约63亿美元）和最初的市场估值（1040亿美元）之间会产生如此巨大的差距呢？目前还没有很好的方法能解释这一点。然而人们普遍开始认为，通过查看公司“账面价值”（大部分是有形资产的价值）来确定企业价值的方法，已经不能充分反映公司的真正价值。事实上，账面价值与“市场价值”（即公司被买断是在股票市场上所获的价值）之间的差距在这几十年中一直在不断地扩大。美国参议院甚至在2000年举行了关于将现行财务报告模式现代化的听证会。现行财务报告模式始于20世纪30年代，当时信息类的企业几乎不存在。现行财务报表模式与现状的差异不仅会影响公司的资产负债表，如果不能正确评估企业的价值，还可能会给企业带来经营风险和市场波动。

    公司账面价值和市场价值之间的差额被记为“无形资产”。20世纪80年代中期，无形资产在美国上市公司市值中约占40%，而在2002年，这一数字已经增长为75%。无形资产早期仅包含品牌、人才和战略这些应计入正规金融会计制度的非有形资产部分。但渐渐地，公司所持有和使用的数据也渐渐纳入了无形资产的范畴。

    最终，这意味着目前还找不到一个有效的方法来计算数据的价值。facebook开盘当天，其正规金融资产与其未记录的无形资产之间相差了近1000亿美元，差距几乎是20倍！太可笑了。但是，随着企业找到资产负债表上记录数据资产价值的方法，这样的差距有一天也必将消除。

    人们正在朝着这个方向前进。在美国最大的无线运营商之一工作的一位高级管理人员透『露』说，数据持有人在认识到数据的巨大价值之后会研究是否在正式的会计条款中将其作为企业的资产。但是，一旦公司的律师得知此事，便会加以阻止。因为把数据计入账面价值可能会使该公司承担法律责任，律师们并不认为这是一个好主意。

    同时，投资者也开始注意到数据的潜在价值。拥有数据或能够轻松收集数据的公司，其股价会上涨；而其他不太幸运的公司，就只能眼看着自己的市值缩水。因为这种状况，数据并不要求其价值正式显示在资产负债表中。尽管做起来有困难，市场和投资者还是会给这些无形资产估价，所以facebook的股价在最初的几个月中一直摇摆不定。但是随着会计窘境和责任问题得到缓解，几乎可以肯定数据的价值将显示在企业的资产负债上，成为一个新的资产类别。

    那么，如何给数据估值呢？诚然，计算价值不再是将其基本用途简单地加总。但是如果数据的大部分价值都是潜在的，需要从未知的二次利用提取，那么人们目前尚不清楚应该如何估算它。这个难度类似于在20世纪70年代布莱克-舒尔斯期权定价理论出现前金融衍生品的定价。它也类似于为专利估值，因为随着各种拍卖、交流、私人销售、许可和大量诉讼的出现，一个知识市场正在逐渐兴起。如果不出意外，给数据的潜在价值贴上价格标签会给金融部门带来无限商机。

    一个办法是从数据持有人在价值提取上所采取的不同策略入手，最常见的一种可能『性』就是将数据授权给第三方。在大数据时代，数据持有人倾向于从被提取的数据价值中抽取一定比例作为报酬支付，而不是敲定一个固定的数额。这有点类似于出版商从书籍、音乐或电影的获利中抽取一定比例，作为支付给作者和表演者的特许权使用费；也类似于生物技术行业的知识产权交易，许可人要求从基于他们技术成果的所有后续发明中抽取一定比例的技术使用费。这样一来，各方都会努力使数据再利用的价值达到最大。然而，由于被许可人可能无法提取数据全部的潜在价值，因此数据持有人可能还会同时向其他方授权使用其数据，两边下注以避免损失。因而，“数据滥交”可能会成为一种常态。

    一些试图给数据定价的市场如雨后春笋般出现。2008年在冰岛成立的datamarket向人们提供其他机构（如联合国、世界银行和欧盟统计局等）的免费数据集，靠倒卖商业供应商（如市场研究公司）的数据来获利。另一家新创办的公司infochimps，其总部设在得克萨斯州奥斯汀市，希望成为一个信息中间人，供第三方以免费或付费的方式共享他们的数据。就像易趣给人们提供了一个出售家中搁置不用的物品的平台一样，这些科技创业公司想为任何手中拥有数据的人提供一个出售数据的平台。例如，鼓励公司授权别人使用自己手中的数据，不然别人也可以从网上免费收集到这些数据。谷歌的前员工吉尔·埃尔巴兹（gil elbaz）创办的factual收集数据，然后制成数据库供需要者使用。

    微软也带着它的windows azure datamarket登上了历史舞台。它的目标是专注高质量的数据和监督所提供的产品，其方式和苹果公司监督其应用程序商店中的产品类似。微软假设，一位销售主管在准备excel表格时可能还需要做一份公司内部数据和来自经济顾问的gdp增长预测的交叉表，那么她只要点击想要购买的数据，后者将瞬间出现在她的电脑屏幕上。

    到目前为止，没有人知道估值模型将发挥出怎样的作用。但可以肯定的是，经济正在渐渐开始围绕数据形成，很多新玩家可以从中受益，而一些资深玩家则可能会找到令人惊讶的新生机。用硅谷技术专家和科技出版社员工蒂姆·奥莱利（tim o’reilly）的话来说就是，“数据是一个平台”，因为数据是新产品和新商业模式的基石。

     

第二部分 大数据时代的商业变革 06 角色定位：数据、技术与思维的三足鼎立

    大数据，决定企业竞争力

    大数据成为许多公司竞争力的来源，从而使整个行业结构都改变了。当然，每个公司的情况各有不同。大公司和小公司最有可能成为赢家，而大部分中等规模的公司则可能无法在这次行业调整中尝到甜头。

    虽然像亚马逊和谷歌一样的行业领头羊会一直保持领先地位，但是和工业时代不一样，它们的企业竞争力并不是体现在庞大的生产规模上。已经拥有的技术设备固然很重要，但那也不是它们的核心竞争力，毕竟如今已经能够快速而廉价地进行大量的数据存储和处理了。公司可以根据实际需要调整它们的计算机技术力量，这样就把固定投入变成了可变投入，同时也削弱了大公司的技术储备规模的优势。

    大规模向小数据时代的赢家以及那些线下大公司（如沃尔玛、联邦快递、宝洁公司、雀巢公司、波音公司）提出了挑战，后者必须意识到大数据的威力然后有策略地收集和使用数据。同时，科技创业公司和新兴产业中的老牌企业也准备收集大量的数据。

    在过去十年里，航空发动机制造商劳斯莱斯通过分析产品使用过程中收集到的数据，实现了商业模式的转型。坐落在英格兰德比郡的劳斯莱斯运营中心一直在监控者全球范围内超过3700架飞机的引擎运行情况，为的就是能在故障发生之前发现问题。数据帮助劳斯莱斯把简单的制造转变成了有附加价值的商业行为：劳斯莱斯出售发动机，同时通过按时计费的方式提供有偿监控服务（一旦出现问题，还进一步提供维修和更换服务）。如今，民用航空发动机部门大约70%的年收入都是来自其提供服务所赚得的费用。

    大数据也为小公司带来了机遇，用埃里克教授的话说就是，聪明而灵活的小公司能享受到非固有资产规模带来的好处。这也就是说，它们可能没有很多的固定资产但是存在感非常强，也可以低成本地传播它们的创新成果。重要的是，因为最好的大规模数据服务都是以创新思维为基础的，所以它们不一定需要大量的原始资本投入。数据可以授权但是不能被占有，数据分析能在云处理平台上快速而低成本地运行，而授权费用则应从数据带来的利益中抽取一小部分。

    大大小小的公司都能从大数据中获利，这个情况很有可能并不只是适用使用数据的公司，也适用于掌握数据的公司。大数据拥有者想尽办法想增加它们的数据存储量，因为这样能以极小的成本带来更大的利润。首先，它们已经具备了存储和处理数据的基础。其次，数据库的融合能带来特有的价值。最后，数据拥有者如果只需要从一人手中购得数据，那将更加省时省力。不过实际情况要远远复杂得多，可能还会有一群处在另一方的数据拥有者（个人）诞生。因为随着数据价值观的显现，很多人会想以数据拥有者的身份大展身手，他们收集的数据往往是和自身相关的，比如他们的购物习惯，观影习惯，也许还有医疗数据等。

    这使得消费者拥有了比以前更大的权利。消费者可以自行决定把这些数据中的多少授权给哪些公司。当然，不是每个人都只在乎把他的数据卖个高价，很多人愿意免费提供这些数据来换取更好的服务，比如想得到亚马逊更准确的图书推荐。但是对于很大一部分对数据敏感的消费者来说，营销和出售他们的个人信息就像写博客，发twitter信息和在维基百科搜索一样自然。

    然而，这一切的发生不只是消费者意识和喜好的转变所能促成的。现在，无论是消费者授权他们的信息还是公司从个人手中购得信息都还过于昂贵和复杂。这很可能会催生出一些中间商，它们从众多消费者手中购得信息，然后卖给公司。如果成本够低，而消费者又足够信任这样的中间商，那么个人数据市场就有可能诞生，这样个人就成功成为了数据拥有者。美国麻省理工学院媒体实验室的个人数据分析专家桑迪·彭特兰与人一起创办的id3公司已经在致力于让这种模式变为现实。

    只有当这些中间商诞生并开始运营，而数据使用者也开始使用这些数据的时候，消费者才能真正成为数据掌握者。如今，消费者在等待足够的设备和适当的数据中间商的出现，在这之前，他们希望自己披『露』的信息越少越好。总之，一旦条件成熟，消费者就能从真正意义上成为数据掌握者了。

    不过，大数据对中等规模的公司帮助并不大。波士顿咨询公司的资深技术和商业顾问菲利普·埃文斯（philip evans）说，超大型的公司占据了数据优势，比小公司更有规模。但是在大数据时代，一个公司没必要非要达到某种规模才能支付它的生产设备所需投入。大数据公司发现它们可以是一个灵活的小公司并且会很成功（或者会被大数据巨头并购）。

    大数据也会撼动国家竞争力。当制造业已经大幅转向发展中国家，而大家都争相发展创新行业的时候，工业化国家因为掌握了数据以及大数据技术，所以仍然在全球竞争中占有优势。不幸的是，这个优势很难持续。就像互联网和计算机技术一样，随着世界上的其他国家和地区都开始采用这些技术，西方世界在大数据技术上的领先地位将慢慢消失。对于发达国家的大公司来说，好消息就是大数据会加剧优胜劣汰。所以一旦一个公司掌握了大数据，它不但可能超过它的对手，还有可能遥遥领先。

    不过，就算有那么多好处，我们依然有担忧的理由。因为随着大数据能够越来越精细的预测世界的事情以及我们所处的位置，我们可能还没有准备好接受它对我们的隐私和决策过程带来的影响。我们的认知和制度都还不习惯这样一个数据充裕的时代，因为它们都建立在数据稀缺的基础之上。下一个章节，我们将探讨大数据所带来的不良影响。

     

第三部分 大数据时代的管理变革 07 风险：让数据主宰一切的隐忧

    我们的隐私被二次利用了

    我们倾向于从数字数据的增长和奥威尔写《1984》时所处“监视炼狱”的角度去理解大数据给个人隐私带来的威胁。但是事实上，不是所有的数据都包含了个人信息。其实，不管是传感器从炼油厂采集的数据、来自工厂的机器数据、机场的气象数据，还是沙井盖爆炸数据都不包含个人信息。英国石油公司和纽约爱迪生联合电力公司不需要（也不想要）个人信息，就能分析挖掘出他们所需要的数据价值。事实上，这方面的数据分析并不威胁个人隐私。

    当然，目前所采集的大部分数据都包含有个人信息，而且存在着各种各样的诱因，让我们想尽办法去采集更多、存储更久、利用更彻底，甚至有的数据表面上并不是个人数据，但是经由大数据处理之后就可以追溯到个人了。

    比方说，如今在美国和欧洲部署的一些智能电表每6秒钟采集一个实时读数，这样一天所得到的数据比过去传统电表收集到的所有数据还要多。因为每个电子设备通电时都会有自己独特的“负荷特征”，比如热水器不同于电脑，而它们与led大麻生长灯又不一样，所以能源使用情况就能暴『露』诸如一个人的日常习惯、医疗条件和非法行为这样的个人信息。（led大麻生长灯，是一种植物补光灯，也是植物生长灯的一种，依照植物生长需要太阳光的规律，代替阳光给植物提供更好的生长发育环境——编者注）

    然而，我们要探讨的主要是大数据是否改变了这种威胁的『性』质，而不是是否加剧了这种威胁。如果仅仅是加剧了这种威胁，那么我们现在采用的保护隐私的法律法规依然是有效的，我们只需要付出加倍的努力来确保有效『性』就可以。然而，倘若威胁的『性』质已经改变了，我们就需要寻求新的解决方案。

    不幸的是，我们的担忧一语中的。大数据的价值不再单纯来源于它的基本用途，而更多源于它的二次利用。这就颠覆了当下隐私保护法以个人为中心的思想：数据收集者必须告知个人，他们收集了哪些数据、作何用途，也必须在收集工作开始之前征得个人的同意。虽然这不是进行合法数据收集的唯一方式，“告知与许可”已经是世界各地执行隐私政策的共识『性』基础（虽然实际上很多的隐私声明都没有达到效果，但那是另一回事）。

    更重要的是，大数据时代，很多数据在收集的时候并无意用作其他用途，而最终却产生了很多创新『性』的用途。所以，公司无法告知个人尚未想到的用途，而个人亦无法同意这种尚是未知的用途。但是只要没有得到许可，任何包含个人信息的大数据分析都需要向个人征得同意。因此，如果谷歌要使用检索词预测流感的话，必须征得数亿用户的同意，这简直无法想象。就算没有技术障碍，又有哪个公司能负担得起这样的人力物力支出呢？

    同样，一开始的时候就要用户同意所有可能的用途，也是不可行的。因为这样一来，“告知与许可”就完全没有意义了。大数据时代，告知与许可这个经过了考验并且可信赖的基石，要么太狭隘，限制了大数据潜在价值的挖掘，要么就太空泛而无法真正地保护个人隐私。

    同时，想在大数据时代中用技术方法来保护隐私也是天方夜谭。如果所有人的信息本来都已经在数据库里，那么有意识地避免某些信息就是此地无银三百两。我们把谷歌街景作为一个例子来看，谷歌的图像采集车在很多国家采集了道路和房屋的图像（以及很多备受争议的数据）。但是，德国媒体和民众强烈地抗议了谷歌的行为，因为民众认为这些图片会帮助黑帮窃贼选择有利可图的目标。有的业主不希望他的房屋或花园出现在这些图片上，顶着巨大的压力，谷歌同意将他们的房屋或花园的影像模糊化。但是这种模糊化却起到了反作用，因为你可以在街景上看到这种有意识的模糊化，对盗贼来说，这又是一个此地无银三百两的例子。

    另一条技术途径在大部分情况下也不可行，那就是匿名化。匿名化指的是让所有能揭示个人情况的信息都不出现在数据集里，比方说名字、生日、住址、信用卡号或者社会保险号等。这样一来，这些数据就可以在被分析和共享的同时，不会威胁到任何人的隐私。在小数据时代这样确实可行，但是随着数据量和种类的增多，大数据促进了数据内容的交叉检验。

    2006年8月，美国在线（aol）公布了大量的旧搜索查询数据，本意是希望研究人员能够从中得出有趣的见解。这个数据库是由从3月1日到5月31日之间的65.7万用户的2000万搜索查询记录组成的，整个数据库进行过精心的匿名化——用户名称和地址等个人信息都使用特殊的数字符号进行了代替。这样，研究人员可以把同一个人的所有搜索查询记录联系在一起来分析，而并不包含任何个人信息。

    尽管如此，《纽约时报》还是在几天之内通过把“60岁的单身男『性』”、“有益健康的茶叶”、“利尔本的园丁”等搜索记录综合分析考虑后，发现数据库中的4417749号代表的是佐治亚州利尔本的一个62岁寡『妇』塞尔玛·阿诺德（thelma arnold）。当记者找到她家的时候，这个老人惊叹道：“天呐！我真没想到一直有人在监视我的私人生活。”这引起了公愤，最终美国在线的首席技术官和另外两名员工都被开除了。

    事隔仅仅两个月之后，也就是2006年10月，dvd租赁商奈飞公司做了一件差不多的事，就是宣布启动“netflix prize”算法竞赛。该公司公布了大约来自50万用户的一亿条租赁记录，并且公开悬赏100万美金，举办一个软件设计大赛来提高他们的电影推荐系统的准确度，胜利的条件是把准确度提高10%。同样，奈飞公司也对数据进行了精心的匿名化处理。然而还是被一个用户认出来了，一个化名“无名氏”的未出柜的同『性』恋母亲起诉了奈飞公司，她来自保守的美国中西部。

    通过把奈飞公司的数据与其他公共数据信息对比分析，得克萨斯大学的研究人员很快发现，匿名用户进行的收视率排名与互联网电影数据库（imdb）上实名用户所排的是匹配的。

    在美国在线的案例中，我们被我们所搜索的内容出卖了。而奈飞公司的情况则是因为不同来源数据的结合暴『露』了我们的身份。这两种情况的出现，都是因为公司没有意识到匿名化对大数据的无效『性』。而出现这种无效『性』则是由两个因素引起的，一是我们收集到的数据越来越多，二是我们会结合越来越多不同来源的数据。

    科罗拉多大学的法学教授保罗·欧姆（paul ohm），同时也是研究反匿名化危害的专家，认为针对大数据的反匿名化，现在还没有很好的办法。毕竟，只要有足够的数据，那么无论如何都做不到完全的匿名化。更糟的是，最近的研究表明，不只是传统数据容易受到反匿名化的影响，人们的社交关系图，也就是人们的相互联系也将同受其害。

    与25年之前的民主德国相比，现在我们所受的监控没有减少，反而变得越来越容易、严密以及低成本。采集个人数据的工具就隐藏在我们日常生活所必备的工具当中，比如网页和智能手机应用程序。我们知道大多数的汽车中都装了一个“黑盒子”——用来监测安全气囊激活的情况，而如今，一旦出现具有争议的交通案件，这个黑盒子所采集的数据就可以在法庭上充当证据。当然，如果企业采集数据只是来提高绩效，我们就不用像被stasi窃听那样而感到那么害怕。毕竟企业再强大，也不如国家强制力。

    不过，即使它们不具备国家强制力，想到各种各样的公司在我们不知情的情况下采集了我们日常生活方方面面的数据，并且进行了数据共享以及一些我们未知的运用，这还是很恐怖的。对大数据大加利用的不只是私营企业，『政府』也不甘落后。

    据《华盛顿邮报》2010年的研究表明，美国国家安全局每天拦截并存储的电子邮件、电话和其他通信记录多达17亿条。前美国安全局官员威廉·宾尼（william binney）估计『政府』采集的美国及他国公民的通信互动记录有20万亿次之多，其中包括谁和谁通过话、发过电子邮件、进行过电汇等信息。为了弄明白这所有的数据，美国建立了庞大的数据中心，其中美国国家安全局就耗资12亿美元在犹他州的威廉姆斯堡建立了一个。

    如今，不再只是负责反恐的秘密机关需要采集更多的数据，所有的『政府』部门都需要，所以，数据采集扩展到了金融交易、医疗记录和facebook状态更新等各个领域，数据量之巨可想而知。『政府』其实处理不了这么多数据，那为什么要费力采集呢？

    这是因为在大数据时代，监控的方式已经改变了。过去，调查员为了尽可能多地知道嫌疑人的信息，需要把鳄鱼夹夹到电话线上。当时最重要的是能深入调查某个人，而现在情况不一样了，比如谷歌和facebook的理念则是人就是社会关系、网上互动和内容搜索的加和。所以，为了全面调查一个人，调查员需要得到关于这个人的最广泛的信息，不仅是他们认识的人，还包括这些人又认识哪些人等。过去的技术条件没法做到这样的分析，但是今非昔比了。

    不过，虽然企业和『政府』拥有的这种采集个人信息的能力，让我们感到很困扰，但也还是没有大数据所引起的另一个新问题让我们更恐慌，那就是用预测来判断我们。

    挣脱大数据的困境

    大数据为监测我们的生活提供了便利，同时也让保护隐私的法律手段失去了应有的效力。面对大数据，保护隐私的核心技术不再适用了。同样，通过大数据的预测，对我们的未来想法而非实际行为采取惩罚措施，也让我们惶恐不安，因为这否认了自由意志并伤害了人类尊严。

    同时，那些尝到大数据益处的人，可能会把大数据运用到它不适用的领域，而且可能会过分膨胀对大数据分析结果的信赖。随着大数据预测的改进，我们会越来越想从大数据中掘金，最终导致一种盲目崇拜，毕竟它是如此的无所不能。这就是我们必须从麦克纳马拉的故事中引以为戒的。

    必须杜绝对数据的过分依赖，以防我们重蹈伊卡洛斯的覆辙。他就是因为过分相信自己的飞行技术，最终误用了数据而落入了海中。下一章，我们将探讨如何让数据为我们所用，而不让我们成为数据的奴隶。

     

第三部分 大数据时代的管理变革 08 掌控：责任与自由并举的信息管理

    一场管理规范的变革

    我们在生产和信息交流方式上的变革必然会引发自我管理所用规范的变革。同时，这些变革也会带动社会需要维护的核心价值观的转变。我们以印刷机的发明导致的信息洪流为例。

    1450年前后，古登堡发明了活字印刷机，在这之前，思想的传播受到了极大的限制。一方面，书籍大多被封禁在修道院的图书馆里，依照天主教精心制定的规定，被僧侣严格看守着，为的是确保并维护其统治地位。在教堂之外，少数几所大学也收藏了一些书籍，大概几百本的样子；15世纪初，剑桥大学图书馆大概有122本大部头。另一方面，读写水平欠缺也是当时信息传播受限的一个重要因素。

    古登堡的印刷机让书籍和手册的大量刊印成为可能。马丁·路德（martin luther）把拉丁语版本的《圣经》翻译成日常使用的德文，让越来越多的人可以不通过牧师而直接聆听上帝的声音，德语版的《圣经》是当时卖得最好的书，这也让他更确信《圣经》可以印刷、分发给成千上万的人。就这样，信息传播越来越广泛。

    这种巨变也使得创立新规范来管理活字印刷术所引发的信息爆炸的条件变得成熟。审查和许可条例被创立，用来规范和管理出版物。著作权法的制定为创作者带来了进行创作的法律和经济动力。随后，保护公民言论自由被写入了宪法。一如既往，权利伴随着责任产生了。当低俗的报纸践踏人们隐私权或诽谤其名誉时，法律规范就会出现以保护人们的隐私权并允许他们对文字诽谤提出上诉。

    可是，变革并不止于规范。这种管理规范上的改变也体现了当时更深层次的价值观转变。在古登堡时期，人类第一次意识到了文字的力量；最终，也意识到了信息广泛传播的重要『性』。几个世纪过去了，我们选择获取更多的信息而非更少，并且借助限制信息滥用的规范而不是最初的审查来防止其泛滥。

    随着世界开始迈向大数据时代，社会也将经历类似的地壳运动。在改变我们许多基本的生活和思考方式的同时，大数据早已在推动我们去重新考虑最基本的准则，包括怎样鼓励其增长以及遏制其潜在威胁。然而，不同于印刷革命，我们没有几个世纪的时间去慢慢适应，我们也许只有几年时间。

    大数据时代，对原有规范的修修补补已经满足不了需要，也不足以抑制大数据带来的风险——我们需要全新的制度规范，而不是修改原有规范的适用范围。想要保护个人隐私就需要个人数据处理器对其政策和行为承担更多的责任。同时，我们必须重新定义公正的概念，以确保人类的行为自由（也相应地为这些行为承担责任）。新机构和专家们需要设计复杂的程序对大数据进行解读，挖掘出其潜在的价值和结论。他们也要向那些可能受害于大数据结论的人——因之被剥夺了工作、接受医疗或贷款权利的人，提供支持。对已有的规范进行修修补补已经不够了，我们需要推陈出新。

     

第三部分 大数据时代的管理变革 结语：正在发生的未来

    大数据时代，名副其实的“信息社会”

    大数据在实用层面的影响很广泛，解决了大量的日常问题。大数据更是利害攸关的，它将重塑我们的生活、工作和思维方式。在某些方面，我们面临着一个僵局，比其他划时代创新引起的社会信息范围和规模急剧扩大所带来的影响更大。我们脚下的地面正在移动。过去确定无疑的事情正在受到质疑。大数据需要人们重新讨论决策、命运和正义的『性』质。我们的世界观正受到相关『性』优势的挑战。拥有知识意味着掌握过去，现在则更意味着能够预测未来。

    当我们准备开发电子商务、寓生活于互联网、进入计算机时代或者拿起算盘时，这些事情比那些代表他们的问题更加重要。我们寻找原因的想法可能被高估了，很多情况下，弄清楚“是什么”比找寻“为什么”更加重要，因为前者表面事实才是我们生活和思维的基础。这些问题可能没有答案。或许，它们是关于人在宇宙中的位置以及能否在喧嚣混『乱』、不可理喻的世界中寻找到意义这一永恒争论的一部分。

    最终，大数据标志着“信息社会”终于名副其实。我们收集的所有数字信息现在都可以用新的方式加以利用。我们可以尝试新的事物并开启新的价值形式。但是，这需要一种新的思维方式，并将挑战我们的社会机构，甚至挑战我们的认同感。可以肯定的是，数据量将继续增长，处理这一切的能力也是如此。但是，现在大多数人都认为大数据是一个技术问题，应侧重于硬件或软件，而我们认为应当更多地考虑当数据说话时会发生什么。

    除了纠结于数据的准确『性』、正确『性』、纯洁度和严格度之外，我们也应该容许一些不精确的存在。数据不可能是完全对或完全错的。当数据的规模以数量级增加时，这些混『乱』也就算不上问题了。事实上，它甚至可以是有好处的，因为当我们只想使用一小部分时，无须捕捉这么多的知识细节。又因为我们可以用更快更便宜的方式找到数据的相关『性』，并且效果往往更好，而不必努力去寻找因果关系。当然在某些情况下，我们仍然需要精心策划的数据来做因果关系研究和控制实验，如测试『药』物的副作用或设计关键的飞机部件。但是在日常情况下，知道“是什么”就已经足够了，不必非要弄清楚“为什么”。大数据的相关『性』将人们指向了比探讨因果关系更有前景的领域。

    这些相关『性』能让我们节省机票钱和预测流感爆发，并知道在一个资源有限的世界中应该检查哪些沙井盖和过度拥挤的建筑物。它可以帮助健康保险公司不做体检就能决定保险覆盖面，并降低提醒病人服『药』的成本。通过大数据的相关『性』，语言可以得到翻译，汽车可以在预测的基础上自行驾驶。沃尔玛可以了解飓风前应在门店准备哪种口味的蛋挞。当然，如果能从中得到因果关系更好。问题是，因果关系往往更难找到，通常我们认为找到了的时候，都是在自欺欺人。

    我们之所以能做所有这些事，新工具只是个很小的因素，无论是更快的处理器、更多的存储器，还是更智能的软件和算法。这些固然重要，但是更为根本的原因是我们拥有了更多的数据，继而世界上更多的事物被数据化了。诚然，人类量化世界的雄心陷于计算机革命，但是数字工具将数据化提升到了新的高度。不仅移动电话能够跟踪到我们呼叫的人和我们所在的位置，而且同样的数据也能用于断定我们是否生病了。不久之后，它或许还能够辨别我们是否恋爱了。

    能置身于信息流中央并且能收集数据的公司通常会繁荣兴旺。有效利用大数据需要专业技术和丰富的想象力，即一个能够容纳大数据的心态，但价值的核心归功于数据本身。有时，重要的资产并不仅仅是能清楚看到的信息，更是从人们与信息交互中收集到的数据废气，聪明的公司可以用它来改善现有的服务，或推出全新的服务。

    大数据同时也给我们带来了巨大的风险。它使得目前用以保护隐私的法律手段和核心技术失去了效果。过去个人身份信息包含的名字、社会安全号码、税收记录等，其构成简单明了。因此隐私保护相对比较简单，只要确保不使用这些信息即可。而今天，即使是最无害的数据，只要被数据收集器采集到足够的量，也会暴『露』出个人身份。匿名化或者是单纯隐藏已不再适用。不仅如此，现在要是对某人进行监督，必定会侵犯到较之以往范围更广的个人隐私内容。因为『政府』在管理上不仅要求个人信息尽可能完善，还记录了其所有的社会关系、交往和交流信息。

    无论大数据如何威胁到隐私保护，最让人们头疼的都是行为倾向问题。大数据预测的准确『性』越来越高，它能预测行为的发生，在人们犯错之前，提前惩处。因为预测的结果几乎不可反驳，人们也就无法为自己开脱。但这种基于预测得出的惩罚不仅违背自由意志的原则，同时也否定了人们会突然改变选择的可能『性』（无论可能『性』有多小）。当我们给一个人判定责任（并给予惩罚）时，必须牢记人类意志的神圣不可侵犯『性』。人类的未来必须保留部分空间，允许我们按照自己的愿望进行塑造。否则，大数据将会扭曲人类最本质的东西，即理『性』思维和自由选择。

    应对大数据的汹涌来袭，我们没有万无一失的方法，必须建立规范自身的新准则。随着社会越来越熟悉大数据的特征和缺陷，我们可以改变一系列的惯『性』来帮助社会应对这种冲击。我们需要把进行隐私保护的责任从个人转移到数据使用者身上，也就是说，数据使用者应该以负责任的态度使用数据。

    在一个预测的时代里，人类的自由意志神圣而不可侵犯，这一点不可轻视。我们不仅需要承认个人进行道德选择的能力，还要强调个人应为自我行为承担责任。社会则必须采取新的保护措施：接受一种新的职业人，也就是数据算法师，对大数据进行深度分析。如此，因为大数据而变得可预测的世界，才不会陷入一个用一种未知取代另一种未知的困境中，不会变成一个黑匣子。

    大数据将成为理解和解决当今许多紧迫的全球问题所不可或缺的重要工具。例如要应对气候变化问题时，需要对污染相关数据进行分析，得出最佳方案，来指导努力方向，找出缓解问题的方法。全球范围内遍布的大量传感设备，包括智能手机内部的传感器，使我们能够以更高的细节水平模拟环境。而世界贫困人口迫切需要提高医疗保健服务，降低医疗费用，这很大程度上可以靠自动化来实现。当下许多似乎需要人类判断才能进行的事情，其实完全可以交由电脑来做，比如癌细胞活检、传染病爆发前期的模拟预测等。

    大数据也被用于发展经济和理解如何预防冲突。基于手机动向数据显示，非洲许多贫民窟地区经济活动十分活跃。大数据还揭示了最可能引发种族关系紧张的社区以及解决难民危机的方式。只有当科技应用至生活的方方面面时，大数据的适用范围才能进一步扩大。

    大数据能帮助我们更好地进行已有的工作，并处理全新的事务。但它绝不是魔术棒，不会带来世界和平，无法根绝贫穷问题，更不能创造出另一个毕加索。大数据不能造婴儿，虽然它确实可以救助早产儿。不要多久，我们将在生活的各个方面使用到大数据，如果不用的话还可能会引起些许焦虑，这种情况就像普通体检查不出问题时，会希望有医生帮我们预约x光进行检查。

    当大数据成为日常生活的一部分后，它将会极大地改变我们对未来的看法。大约五百年前，欧洲在逐渐发展为更加自由、科学、文明的世界的进程中，欧洲人经历了对时间认知的重大转变。在此之前，时间被认为是循环的，生命也是轮转的。每天或每年与过去的日子如出一辙，甚至连生命的终结也与起点相似，因为濒死的成人会显示出孩子的特征。认知转变后，时间变作线『性』的，成了一条岁月演变过程，过程中世界因人变化，生命的轨迹也受到相应的影响。如果说这以前的历史中，过去、当下、未来的概念是完全交织在一起的，那么通过塑造当下，人类现在便有了过去可以回顾，有了未来可以展望。

    虽然我们可以塑造当下，但未来却从过去的“完全可预测”转变为一块开放又原始、广阔而空白的帆布，所有人都可以在上面依据自己的价值，努力裁剪塑形。“现代”的一个定义『性』特征便是人类感到自己是命运的主人，这使我们与生活在宿命论桎梏中的先辈们截然不同。但是大数据预测却又使我们的生命帆布不再那么开放、原始和纯净。对于善于运用科技解读未来的人来说，我们的未来不再是只字未书的画布，而是似乎已经着上了淡淡的墨痕。未来的可预知『性』似乎缩小了塑造命运的空间。潜在的可能『性』在概念的圣坛上被解剖。

    与此同时，大数据又意味着我们将永远受困于过去的行为，这些行为在预知我们下一步的预测过程中与我们作对，即我们永远无法逃避已发生的事。莎士比亚曾写道：“凡是过去，皆为序曲。”大数据通过运算将这句话铭刻，无论结果好坏——无论这句话是否会浇熄我们迎接下一个日出的热情，是否会打击我们留名于世的渴望。

    其实，事实很多可能是相反的。知道行为在未来如何谢幕，我们便可以采取补救措施，避免问题发生并改善结局。我们能在期末考试之前早早发现有退步趋势的学生。我们能检测出微小的癌变，赶在疾病完全爆发前根治。我们能看到青春期意外妊娠的可能『性』，或是预测到某种犯罪生涯，然后尽力干预，避免出现可能的悲剧结局。例如拥挤的纽约住宅着火的时候，如果能事先知道并从几间最可能是火源的公寓着手，将会免除一场致命的火灾。

    没有什么是上天注定的，因为我们总能就手中的信息制定出相应的对策。大数据预测结果也并非铁定，而只是提供了一种可能『性』，也就是说，只要我们愿意，结局可以改写。我们可以判断出迎接未来的最佳方式，摇身变作未来的主人，正如莫里在海与风的广阔世界中乘风破浪一般。在过程中我们无须理解宇宙的奥秘或是去证明神的存在，因为大数据已经帮我们做好了。
